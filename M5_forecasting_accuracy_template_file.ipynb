{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5_forecasting_accuracy_template_file.ipynb",
      "provenance": [],
      "mount_file_id": "1VDSwHYPIIpS74UVVP_wa_fxnTzhkEOP3",
      "authorship_tag": "ABX9TyPXXbYiAImkDDyiTxkWPCEa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalchapagain/Kaggle_competition_solutions/blob/master/M5_forecasting_accuracy_template_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtH1rObKSgMv"
      },
      "source": [
        "# Summary of this notebook\n",
        "\n",
        "**This notebook develope the M5-accuracy model, it is divided into four main parts **\n",
        "- Part 1. Simple Feature Engineering,\n",
        "    - Sampling of dataset\n",
        "    - Downcast of memory\n",
        "    - Melting\n",
        "    - Statistical features of sales: min/max/mean/std\n",
        "    - Normalization (min/max scaling)\n",
        "    - rolling aggregation\n",
        "    - momentum\n",
        "- Part 2. Lag Analysis,\n",
        "- Part 3: Mean encoding (Custom encoding)\n",
        "    - FE creation approaches\n",
        "    - Baseline model for FE validation\n",
        "    - Implementation of PCA for Dimension reduction\n",
        "    - FE validation by Permutation importance\n",
        "\n",
        "- Part 4: Final forecasting\n",
        "    - parallelization for FE\n",
        "    - get the final_processed datasets\n",
        "    - model selection (implementation)\n",
        "    - Train the model (beware of data leakage)\n",
        "    - Final forecasting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaOXY1ABSbCa"
      },
      "source": [
        "# Part 1. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3RbElbFUEp3"
      },
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzOfwtXWUE2l"
      },
      "source": [
        "## Simple \"Memory profilers\" to see memory usage\n",
        "def get_memory_usage():\n",
        "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
        "        \n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qMXXZ_fcR4N"
      },
      "source": [
        "# Part 1.1 Downcating the memory "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbFqHZZTUFCY"
      },
      "source": [
        "## Downcating the memory \n",
        "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
        "# :verbose                                        # type: bool\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                       df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVBdLlGGUFLF"
      },
      "source": [
        "## Merging by concat to not lose dtypes\n",
        "def merge_by_concat(df1, df2, merge_on):\n",
        "    merged_gf = df1[merge_on]\n",
        "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
        "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
        "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
        "    return df1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-lI7a7QU3rB"
      },
      "source": [
        "########################### Vars ##############\n",
        "TARGET = 'sales'         # Our main target\n",
        "END_TRAIN = 1913         # Last day in train set, However new dataset contains upto d_1941\n",
        "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF5-Nk9jfEWk"
      },
      "source": [
        "**load dataset from location**\n",
        "- path='/content/drive/My Drive/GoogleColab/'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad_4wbYoZ68W",
        "outputId": "1869d26c-f20d-4857-83ef-4aa371dcb06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "########################### Load Data#####################\n",
        "print('Loading the sampled Data')\n",
        "path='/content/drive/My Drive/GoogleColab/'\n",
        "train_df = pd.read_csv(path+'sales_train_evaluation.csv')\n",
        "prices_df = pd.read_csv(path+'sell_prices.csv')\n",
        "calendar_df =pd.read_csv(path+'calendar.csv')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the sampled Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGVr3RW1ZUei",
        "outputId": "9f790251-0b73-40cd-8cd4-e9d35651fa16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "train_df.head(1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d_1</th>\n",
              "      <th>d_2</th>\n",
              "      <th>d_3</th>\n",
              "      <th>d_4</th>\n",
              "      <th>d_5</th>\n",
              "      <th>d_6</th>\n",
              "      <th>d_7</th>\n",
              "      <th>d_8</th>\n",
              "      <th>d_9</th>\n",
              "      <th>d_10</th>\n",
              "      <th>d_11</th>\n",
              "      <th>d_12</th>\n",
              "      <th>d_13</th>\n",
              "      <th>d_14</th>\n",
              "      <th>d_15</th>\n",
              "      <th>d_16</th>\n",
              "      <th>d_17</th>\n",
              "      <th>d_18</th>\n",
              "      <th>d_19</th>\n",
              "      <th>d_20</th>\n",
              "      <th>d_21</th>\n",
              "      <th>d_22</th>\n",
              "      <th>d_23</th>\n",
              "      <th>d_24</th>\n",
              "      <th>d_25</th>\n",
              "      <th>d_26</th>\n",
              "      <th>d_27</th>\n",
              "      <th>d_28</th>\n",
              "      <th>d_29</th>\n",
              "      <th>d_30</th>\n",
              "      <th>d_31</th>\n",
              "      <th>d_32</th>\n",
              "      <th>d_33</th>\n",
              "      <th>d_34</th>\n",
              "      <th>...</th>\n",
              "      <th>d_1902</th>\n",
              "      <th>d_1903</th>\n",
              "      <th>d_1904</th>\n",
              "      <th>d_1905</th>\n",
              "      <th>d_1906</th>\n",
              "      <th>d_1907</th>\n",
              "      <th>d_1908</th>\n",
              "      <th>d_1909</th>\n",
              "      <th>d_1910</th>\n",
              "      <th>d_1911</th>\n",
              "      <th>d_1912</th>\n",
              "      <th>d_1913</th>\n",
              "      <th>d_1914</th>\n",
              "      <th>d_1915</th>\n",
              "      <th>d_1916</th>\n",
              "      <th>d_1917</th>\n",
              "      <th>d_1918</th>\n",
              "      <th>d_1919</th>\n",
              "      <th>d_1920</th>\n",
              "      <th>d_1921</th>\n",
              "      <th>d_1922</th>\n",
              "      <th>d_1923</th>\n",
              "      <th>d_1924</th>\n",
              "      <th>d_1925</th>\n",
              "      <th>d_1926</th>\n",
              "      <th>d_1927</th>\n",
              "      <th>d_1928</th>\n",
              "      <th>d_1929</th>\n",
              "      <th>d_1930</th>\n",
              "      <th>d_1931</th>\n",
              "      <th>d_1932</th>\n",
              "      <th>d_1933</th>\n",
              "      <th>d_1934</th>\n",
              "      <th>d_1935</th>\n",
              "      <th>d_1936</th>\n",
              "      <th>d_1937</th>\n",
              "      <th>d_1938</th>\n",
              "      <th>d_1939</th>\n",
              "      <th>d_1940</th>\n",
              "      <th>d_1941</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 1947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id        item_id  ... d_1940 d_1941\n",
              "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  ...      0      1\n",
              "\n",
              "[1 rows x 1947 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep0p-CF2Xjxk"
      },
      "source": [
        "**This this dataset upto 1913 is used for validation purpose, therefor, for simplicity to undersand, we are going to change the column *id* contains *_evaluation into **_validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GBjF2iUZp4a"
      },
      "source": [
        "import re\n",
        "train_df.id.replace({'evaluation':'validation'}, regex=True, inplace=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTF1fddoceYD"
      },
      "source": [
        "# Part 1.2 Sampling of dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5QHcV8-YlZZ"
      },
      "source": [
        "***Due to computational limitations, we are just taking 10% of dataset. Few bloggers talking about 'Stratified sampling', however we implement train_test_split strategy with random state.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPpxzQtbZ6tx",
        "outputId": "700632a1-11f8-4d0a-a992-0294ab455868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Reduction of dataset (just take 10% of dataset)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def sampling_dataset(dataset):\n",
        "  data_keep, data_discard=train_test_split(dataset,train_size=0.1, random_state=42)\n",
        "  print('Total size and sampled size')\n",
        "  print(dataset.shape,data_keep.shape)\n",
        "  data_keep.sort_index(inplace=True)\n",
        "  data_keep.reset_index(drop=True, inplace=True)\n",
        "  return data_keep\n",
        "\n",
        "train_df=sampling_dataset(train_df)\n",
        "prices_df=sampling_dataset(prices_df)\n",
        "train_df_for_lag_feature=train_df.copy()\n",
        "\n",
        "\n",
        "# train_df = sales_keep.copy() #pd.read_pickle(path+'sales_sampled.pkl')\n",
        "# prices_df = prices_keep.copy() #pd.read_pickle(path+'prices_sampled.pkl')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total size and sampled size\n",
            "(30490, 1947) (3049, 1947)\n",
            "Total size and sampled size\n",
            "(6841121, 4) (684112, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtf0Y-2tYZ83",
        "outputId": "242e55b9-9e8d-4dd8-c43b-37eeb5cea917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Garbage collection (memory management)\n",
        "gc.collect()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLJKihMSiotL"
      },
      "source": [
        "# Part 1.3  Melting the dataset\n",
        "- In this section the we transform the horizontal dataset into the vertical view.\n",
        "\n",
        "- Out index will be *'id'*, *'item_id'*, *'dept_id'*, *'cat_id'*, *'store_id'*, *'state_id'* and labels are *'d_'* coulmns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6rRGVbrU3y7",
        "outputId": "e536c783-1202-49ec-eb98-202d556bc115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Making Grids (for prices/sales) ###################################\n",
        "print('Melting the dataset to different grids')\n",
        "\n",
        "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
        "grid_df = pd.melt(train_df, \n",
        "                  id_vars = index_columns, \n",
        "                  var_name = 'd', \n",
        "                  value_name = TARGET)\n",
        "\n",
        "\n",
        "#After melting, lets observe the train rows and columns\n",
        "print('Train rows:', len(train_df), len(grid_df))\n",
        "\n",
        "\n",
        "# To be able to make predictions we need to add \"test set\" to our grid\n",
        "add_grid = pd.DataFrame()\n",
        "for i in range(1,29):\n",
        "    temp_df = train_df[index_columns]\n",
        "    temp_df = temp_df.drop_duplicates()\n",
        "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
        "    temp_df[TARGET] = np.nan\n",
        "    add_grid = pd.concat([add_grid,temp_df])\n",
        "\n",
        "grid_df = pd.concat([grid_df,add_grid])\n",
        "grid_df = grid_df.reset_index(drop=True)\n",
        "\n",
        "# Remove some temoprary DFs\n",
        "del temp_df, add_grid\n",
        "\n",
        "# We will not need original train_df anymore and can remove it\n",
        "del train_df\n",
        "\n",
        "# Let's check our memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Size of grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Melting the dataset to different grids\n",
            "Train rows: 3049 5918109\n",
            "     Size of grid_df: 366.4MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvhqJtbBmei5"
      },
      "source": [
        "*We can free some memory by converting \"strings\" to categorical which will not affect merging and we will not lose any valuable data *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cWm3h9vlnUe",
        "outputId": "da171377-58e6-4c9e-9b8c-4574b668d578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for col in index_columns:\n",
        "    grid_df[col] = grid_df[col].astype('category')\n",
        "\n",
        "# Let's check again memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Reduced grid_df: 137.6MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAGqJ5IHm5uc"
      },
      "source": [
        "- ***The important thing that need to understand is that the leadings zero values in each*** *train_df*  ***item row are not real 0 sales but mean absence for the item in the store. Such zeros can remove to save some memory***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQEP-G43U3_2",
        "outputId": "682a1016-86a2-48f7-fa68-831152bc185e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Product Release date ###############################\n",
        "print('Release week')\n",
        "\n",
        "# Prices are set by week so it we will have not very accurate release week \n",
        "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
        "release_df.columns = ['store_id','item_id','release']\n",
        "\n",
        "# Now we can merge release_df\n",
        "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
        "del release_df\n",
        "\n",
        "# We want to remove some \"zeros\" rows from grid_df to do it we need wm_yr_wk column\n",
        "# let's merge partly calendar_df to have it\n",
        "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
        "                      \n",
        "# Now we can cutoff some rows and safe memory \n",
        "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
        "grid_df = grid_df.reset_index(drop=True)\n",
        "\n",
        "# Let's check our memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "\n",
        "# Should we keep release week as one of the features? for now, lets keep release feature by reducing the memory size int16.\n",
        "\n",
        "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
        "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
        "\n",
        "# Let's check again memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Release week\n",
            "    Original grid_df: 174.9MiB\n",
            "     Reduced grid_df: 148.7MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61ghTbIU39x",
        "outputId": "847e0ac4-b263-44cb-8cf4-8124d31425f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Save part 1 ################\n",
        "print('Save Part 1')\n",
        "print('Part 1 is ready')\n",
        "\n",
        "# We have our BASE grid ready and can save it as pickle file for future use (model training)\n",
        "grid_df.to_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_1.pkl')\n",
        "\n",
        "print('Size:', grid_df.shape)\n",
        "grid_part_1=grid_df.copy() #save in the name of grid_part_1"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Part 1\n",
            "Part 1 is ready\n",
            "Size: (4578794, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuI5GroVa0le",
        "outputId": "f8cea44d-6af7-456d-9ef9-30eebc0e0b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 10 columns):\n",
            " #   Column    Dtype   \n",
            "---  ------    -----   \n",
            " 0   id        category\n",
            " 1   item_id   category\n",
            " 2   dept_id   category\n",
            " 3   cat_id    category\n",
            " 4   store_id  category\n",
            " 5   state_id  category\n",
            " 6   d         object  \n",
            " 7   sales     float64 \n",
            " 8   release   int16   \n",
            " 9   wm_yr_wk  int64   \n",
            "dtypes: category(6), float64(1), int16(1), int64(1), object(1)\n",
            "memory usage: 148.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMIVox9LpjO4"
      },
      "source": [
        "#Part 2. Feature Engineering\n",
        "- **Some basic FE for prices are:** \n",
        "- statistical feature for prices are: min_price, max_price, average price, deviation in price\n",
        "- Normalization (min/max scaling)\n",
        "- \"rolling\" aggregations but would like months and years as \"window\"\n",
        "- Momentum of the prices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU2APJiIV_Q9",
        "outputId": "98d91564-d2f0-4752-c608-e4a7100db4a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "########################### Prices ############################################\n",
        "print('Feature Engineering of Prices')\n",
        "\n",
        "# We can do some basic aggregations\n",
        "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
        "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
        "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
        "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
        "\n",
        "# and do price normalization (min/max scaling)\n",
        "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
        "\n",
        "# Some items are can be inflation dependent and some items are very \"stable\"\n",
        "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
        "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
        "\n",
        "# we would like some \"rolling\" aggregations but would like months and years as \"window\"\n",
        "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
        "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
        "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
        "del calendar_prices\n",
        "\n",
        "# Now we can add price \"momentum\" (some sort of) Shifted by week, by month mean, and by year mean\n",
        "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
        "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
        "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
        "\n",
        "del prices_df['month'], prices_df['year']\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Engineering of Prices\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PIPeaEKV_bp",
        "outputId": "9ade05b0-b1a8-4b8c-96c7-3711ee9064ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Merge prices and save part 2 #################################################\n",
        "print('Merge prices and save part 2')\n",
        "\n",
        "# Merge Prices\n",
        "original_columns = list(grid_df)\n",
        "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
        "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
        "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
        "grid_df = reduce_mem_usage(grid_df)\n",
        "\n",
        "# Safe part 2\n",
        "grid_df.to_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_2.pkl')\n",
        "print('Size:', grid_df.shape)\n",
        "grid_part_2=grid_df.copy()\n",
        "\n",
        "# We don't need prices_df anymore\n",
        "del prices_df\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Merge prices and save part 2\n",
            "Mem. usage decreased to 87.43 Mb (23.1% reduction)\n",
            "Size: (4578794, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYGkPa8pbQyK",
        "outputId": "2a71e9c6-5502-4df5-915e-5a888b800c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "grid_part_2.info()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Dtype   \n",
            "---  ------      -----   \n",
            " 0   id          category\n",
            " 1   d           object  \n",
            " 2   sell_price  float16 \n",
            "dtypes: category(1), float16(1), object(1)\n",
            "memory usage: 87.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HAhEw1zrL46",
        "outputId": "c5eccbaf-2a57-4d1a-e17e-231ba463bda4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "calendar_df.columns"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'd',\n",
              "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
              "       'snap_CA', 'snap_TX', 'snap_WI'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpJl0XJeWGjK"
      },
      "source": [
        "########################### Lets Merge the calendar ###########################\n",
        "\n",
        "grid_df = grid_df[MAIN_INDEX]\n",
        "\n",
        "# Merge calendar partly \n",
        "icols = ['date',\n",
        "         'd',\n",
        "         'event_name_1',\n",
        "         'event_type_1',\n",
        "         'event_name_2',\n",
        "         'event_type_2',\n",
        "         'snap_CA',\n",
        "         'snap_TX',\n",
        "         'snap_WI']\n",
        "\n",
        "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
        "\n",
        "# Again, lets minify data\n",
        "# 'snap_' columns we can convert to bool or int8\n",
        "icols = ['event_name_1',\n",
        "         'event_type_1',\n",
        "         'event_name_2',\n",
        "         'event_type_2',\n",
        "         'snap_CA',\n",
        "         'snap_TX',\n",
        "         'snap_WI']\n",
        "for col in icols:\n",
        "    grid_df[col] = grid_df[col].astype('category')\n",
        "\n",
        "# Convert to DateTime\n",
        "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
        "\n",
        "# Make some features from date\n",
        "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
        "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
        "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
        "grid_df['tm_y'] = grid_df['date'].dt.year\n",
        "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
        "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
        "\n",
        "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
        "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
        "\n",
        "# Remove date\n",
        "del grid_df['date']"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnbPwrMVWGub",
        "outputId": "84f5be1e-55c6-4540-f6da-622ff5b50544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "########################### Save part 3 (Dates) ###########################\n",
        "print('Save part 3')\n",
        "\n",
        "grid_df.to_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_3.pkl')\n",
        "print('Size:', grid_df.shape)\n",
        "\n",
        "grid_part_3=grid_df.copy()\n",
        "\n",
        "del calendar_df\n",
        "del grid_df"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save part 3\n",
            "Size: (4578794, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NUHosBUbhll",
        "outputId": "d406aa5a-ac26-431f-e4c6-7d518c1e5e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "grid_part_3.info()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Dtype   \n",
            "---  ------        -----   \n",
            " 0   id            category\n",
            " 1   d             object  \n",
            " 2   event_name_1  category\n",
            " 3   event_type_1  category\n",
            " 4   event_name_2  category\n",
            " 5   event_type_2  category\n",
            " 6   snap_CA       category\n",
            " 7   snap_TX       category\n",
            " 8   snap_WI       category\n",
            " 9   tm_d          int8    \n",
            " 10  tm_w          int8    \n",
            " 11  tm_m          int8    \n",
            " 12  tm_y          int8    \n",
            " 13  tm_wm         int8    \n",
            " 14  tm_dw         int8    \n",
            " 15  tm_w_end      int8    \n",
            "dtypes: category(8), int8(7), object(1)\n",
            "memory usage: 139.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imtt6-sDV_xH"
      },
      "source": [
        "########################### Some additional cleaning (converting 'd' into int)\n",
        "#################################################################################\n",
        "\n",
        "## Part 1\n",
        "# Convert 'd' to int\n",
        "grid_df = grid_part_1.copy()\n",
        "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
        "\n",
        "path='/content/drive/My Drive/GoogleColab/pickle_files/'\n",
        "grid_df.to_pickle(path+'grid_part_1.pkl')\n",
        "\n",
        "# Remove 'wm_yr_wk'\n",
        "# as test values are not in train set\n",
        "del grid_df['wm_yr_wk']\n",
        "grid_part_1=grid_df.copy()\n",
        "\n",
        "del grid_df\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4MS0zBCicY3"
      },
      "source": [
        "# #Path for pickle file\n",
        "# path='/content/drive/My Drive/GoogleColab/pickle_files/'\n",
        "# grid_part_1.to_pickle(path+'grid_part_1.pkl')\n",
        "# grid_part_2.to_pickle(path+'grid_part_2.pkl')\n",
        "# grid_part_3.to_pickle(path+'grid_part_3.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDYseFKRi7J5"
      },
      "source": [
        "# #Lets read same 3 sets of features\n",
        "# grid_df = pd.concat([pd.read_pickle(path+'grid_part_1.pkl'),\n",
        "#                      pd.read_pickle(path+'grid_part_2.pkl').iloc[:,2:],\n",
        "#                      pd.read_pickle(path+'grid_part_3.pkl').iloc[:,2:]],\n",
        "#                      axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW0gDQM3cTjo",
        "outputId": "f997f714-a718-44a1-afcf-55f9d05e9f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# lets concat all three grids for further processing\n",
        "\n",
        "grid_df=pd.concat([grid_part_1, grid_part_2.iloc[:,2:], grid_part_3.iloc[:,2:]], axis=1)\n",
        "\n",
        "# Let's check again memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "print('Size:', grid_df.shape)\n",
        "\n",
        "#           Full Grid:   157.4 MiB\n",
        "\n",
        "#Still memory usage is high, we can train by state_id or shop_id !!\n",
        "\n",
        "state_id = 'CA'\n",
        "grid_df = grid_df[grid_df['state_id']==state_id]\n",
        "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "# Full Grid:  75.3MiB\n",
        "\n",
        "store_id = 'CA_1'\n",
        "grid_df = grid_df[grid_df['store_id']==store_id]\n",
        "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "#           Now, Full Grid: 19.2 MiB\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Full Grid: 157.4MiB\n",
            "Size: (4578794, 24)\n",
            "           Full Grid:  75.3MiB\n",
            "           Full Grid:  19.2MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70xo3kmxcTss",
        "outputId": "1d97f4a2-4382-4326-e957-e7398f68d29e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "########################### Final list of features\n",
        "#################################################################################\n",
        "grid_df.info()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 451999 entries, 0 to 4576039\n",
            "Data columns (total 24 columns):\n",
            " #   Column        Non-Null Count   Dtype   \n",
            "---  ------        --------------   -----   \n",
            " 0   id            451999 non-null  category\n",
            " 1   item_id       451999 non-null  category\n",
            " 2   dept_id       451999 non-null  category\n",
            " 3   cat_id        451999 non-null  category\n",
            " 4   store_id      451999 non-null  category\n",
            " 5   state_id      451999 non-null  category\n",
            " 6   d             451999 non-null  int16   \n",
            " 7   sales         443739 non-null  float64 \n",
            " 8   release       451999 non-null  int16   \n",
            " 9   sell_price    46182 non-null   float16 \n",
            " 10  event_name_1  37127 non-null   category\n",
            " 11  event_type_1  37127 non-null   category\n",
            " 12  event_name_2  863 non-null     category\n",
            " 13  event_type_2  863 non-null     category\n",
            " 14  snap_CA       451999 non-null  category\n",
            " 15  snap_TX       451999 non-null  category\n",
            " 16  snap_WI       451999 non-null  category\n",
            " 17  tm_d          451999 non-null  int8    \n",
            " 18  tm_w          451999 non-null  int8    \n",
            " 19  tm_m          451999 non-null  int8    \n",
            " 20  tm_y          451999 non-null  int8    \n",
            " 21  tm_wm         451999 non-null  int8    \n",
            " 22  tm_dw         451999 non-null  int8    \n",
            " 23  tm_w_end      451999 non-null  int8    \n",
            "dtypes: category(13), float16(1), float64(1), int16(2), int8(7)\n",
            "memory usage: 19.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp1vZEg2cBJy"
      },
      "source": [
        "# Part 2.1 Encoding (Lag Feature)\n",
        "- Rolling lag\n",
        "\n",
        "https://www.kaggle.com/kyakovlev/m5-lags-features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGjJ92QVU37M"
      },
      "source": [
        "#Lets load the sales dataset\n",
        "train_df=train_df_for_lag_feature.copy() #lets save one file at the begining. This file will be needed in future."
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeq2K03Yk5m0",
        "outputId": "1b81c234-e283-4574-87d4-d2f63dac0df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To make all calculations faster, we will limit dataset by 'CA' state\n",
        "train_df = train_df[train_df['state_id']=='CA']\n",
        "print('Shape is: %s' %str(train_df.shape))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: (1202, 1947)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz-7K8DTySvJ"
      },
      "source": [
        "*** Here the number of features are d_1 to d_1941 (but we will consider upto d_1912) i.e a lot of feature, which is good, but we have just 1202 training rows (however we are taking only for CA) ***\n",
        "\n",
        "*** In other hand we can think of d_ columns as additional labels and can significantly scale up our training set*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqHg6qL3k5zR",
        "outputId": "dd27f397-10e1-421a-993f-caca171408c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "## Horizontal representation to vertical representation (tranformation)\n",
        "train_df.iloc[:10]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d_1</th>\n",
              "      <th>d_2</th>\n",
              "      <th>d_3</th>\n",
              "      <th>d_4</th>\n",
              "      <th>d_5</th>\n",
              "      <th>d_6</th>\n",
              "      <th>d_7</th>\n",
              "      <th>d_8</th>\n",
              "      <th>d_9</th>\n",
              "      <th>d_10</th>\n",
              "      <th>d_11</th>\n",
              "      <th>d_12</th>\n",
              "      <th>d_13</th>\n",
              "      <th>d_14</th>\n",
              "      <th>d_15</th>\n",
              "      <th>d_16</th>\n",
              "      <th>d_17</th>\n",
              "      <th>d_18</th>\n",
              "      <th>d_19</th>\n",
              "      <th>d_20</th>\n",
              "      <th>d_21</th>\n",
              "      <th>d_22</th>\n",
              "      <th>d_23</th>\n",
              "      <th>d_24</th>\n",
              "      <th>d_25</th>\n",
              "      <th>d_26</th>\n",
              "      <th>d_27</th>\n",
              "      <th>d_28</th>\n",
              "      <th>d_29</th>\n",
              "      <th>d_30</th>\n",
              "      <th>d_31</th>\n",
              "      <th>d_32</th>\n",
              "      <th>d_33</th>\n",
              "      <th>d_34</th>\n",
              "      <th>...</th>\n",
              "      <th>d_1902</th>\n",
              "      <th>d_1903</th>\n",
              "      <th>d_1904</th>\n",
              "      <th>d_1905</th>\n",
              "      <th>d_1906</th>\n",
              "      <th>d_1907</th>\n",
              "      <th>d_1908</th>\n",
              "      <th>d_1909</th>\n",
              "      <th>d_1910</th>\n",
              "      <th>d_1911</th>\n",
              "      <th>d_1912</th>\n",
              "      <th>d_1913</th>\n",
              "      <th>d_1914</th>\n",
              "      <th>d_1915</th>\n",
              "      <th>d_1916</th>\n",
              "      <th>d_1917</th>\n",
              "      <th>d_1918</th>\n",
              "      <th>d_1919</th>\n",
              "      <th>d_1920</th>\n",
              "      <th>d_1921</th>\n",
              "      <th>d_1922</th>\n",
              "      <th>d_1923</th>\n",
              "      <th>d_1924</th>\n",
              "      <th>d_1925</th>\n",
              "      <th>d_1926</th>\n",
              "      <th>d_1927</th>\n",
              "      <th>d_1928</th>\n",
              "      <th>d_1929</th>\n",
              "      <th>d_1930</th>\n",
              "      <th>d_1931</th>\n",
              "      <th>d_1932</th>\n",
              "      <th>d_1933</th>\n",
              "      <th>d_1934</th>\n",
              "      <th>d_1935</th>\n",
              "      <th>d_1936</th>\n",
              "      <th>d_1937</th>\n",
              "      <th>d_1938</th>\n",
              "      <th>d_1939</th>\n",
              "      <th>d_1940</th>\n",
              "      <th>d_1941</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_012</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_014_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_014</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_056_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_056</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_062_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_062</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HOBBIES_1_080_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_080</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>HOBBIES_1_092_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_092</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>HOBBIES_1_110_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_110</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>HOBBIES_1_122_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_122</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>HOBBIES_1_130_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_130</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 1947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id        item_id  ... d_1940 d_1941\n",
              "0  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...      0      1\n",
              "1  HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  ...      1      0\n",
              "2  HOBBIES_1_014_CA_1_validation  HOBBIES_1_014  ...      1      3\n",
              "3  HOBBIES_1_056_CA_1_validation  HOBBIES_1_056  ...      0      0\n",
              "4  HOBBIES_1_062_CA_1_validation  HOBBIES_1_062  ...      1      1\n",
              "5  HOBBIES_1_080_CA_1_validation  HOBBIES_1_080  ...      0      0\n",
              "6  HOBBIES_1_092_CA_1_validation  HOBBIES_1_092  ...      0      0\n",
              "7  HOBBIES_1_110_CA_1_validation  HOBBIES_1_110  ...      0      0\n",
              "8  HOBBIES_1_122_CA_1_validation  HOBBIES_1_122  ...      0      4\n",
              "9  HOBBIES_1_130_CA_1_validation  HOBBIES_1_130  ...      1      3\n",
              "\n",
              "[10 rows x 1947 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx7o3Hjb78v1"
      },
      "source": [
        "***Lets go for Vertical representation:***\n",
        "\n",
        "- **In other hand we can think of d_ columns as additional labels and can significantly** \n",
        "- **scale up our training set to 2333082 rows with 8 columns**\n",
        "\n",
        "- **Good thing that our model will have greater input for training**\n",
        "\n",
        "- **Bad thing that we are losing lags that we had in horizontal representation and**\n",
        "- **also new data set consumes much more memory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF1d_wPtk5xJ",
        "outputId": "1bb97cc2-0808-4b6a-9af5-0acc376bd102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
        "train_df = pd.melt(train_df, \n",
        "                  id_vars = index_columns, \n",
        "                  var_name = 'd', \n",
        "                  value_name = TARGET)\n",
        "\n",
        "train_df[train_df['id']=='HOBBIES_1_010_CA_1_validation'].iloc[:10]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3606</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4808</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6010</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7212</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9616</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10818</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id        item_id  ...     d sales\n",
              "0      HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_1     0\n",
              "1202   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_2     0\n",
              "2404   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_3     1\n",
              "3606   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_4     0\n",
              "4808   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_5     0\n",
              "6010   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_6     0\n",
              "7212   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_7     0\n",
              "8414   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_8     0\n",
              "9616   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_9     0\n",
              "10818  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...  d_10     0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYpTI_E8ZnW"
      },
      "source": [
        "*** Lags creation ***\n",
        "- **For the lag creation our dataset is allready sorted by d values**\n",
        "- **now, we can simply shift() values also we have to keep in mind that we need to aggregate values on 'id' level**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5tbIbisk5rH",
        "outputId": "4cec4c44-8d8a-40f5-ebc2-ae78b2745cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "########################### Lags creation ##########################################################\n",
        "\n",
        "# group and shift in loop\n",
        "temp_df = train_df[['id','d',TARGET]]\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(1,8):\n",
        "    print('Shifting:', i)\n",
        "    temp_df['lag_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(i))\n",
        "    \n",
        "print('%0.2f min: Time for loops' % ((time.time() - start_time) / 60))\n",
        "\n",
        "\n",
        "# Or same in \"compact\" manner\n",
        "LAG_DAYS = [col for col in range(1,8)]\n",
        "temp_df = train_df[['id','d',TARGET]]\n",
        "\n",
        "start_time = time.time()\n",
        "temp_df = temp_df.assign(**{\n",
        "        '{}_lag_{}'.format(col, l): temp_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
        "        for l in LAG_DAYS\n",
        "        for col in [TARGET]\n",
        "    })\n",
        "\n",
        "print('%0.2f min: Time for bulk shift' % ((time.time() - start_time) / 60))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shifting: 1\n",
            "Shifting: 2\n",
            "Shifting: 3\n",
            "Shifting: 4\n",
            "Shifting: 5\n",
            "Shifting: 6\n",
            "Shifting: 7\n",
            "0.15 min: Time for loops\n",
            "0.15 min: Time for bulk shift\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzRpS5tQ8yXn",
        "outputId": "59df09e4-f824-4c08-909b-fc1a369b8ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "temp_df"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>sales_lag_1</th>\n",
              "      <th>sales_lag_2</th>\n",
              "      <th>sales_lag_3</th>\n",
              "      <th>sales_lag_4</th>\n",
              "      <th>sales_lag_5</th>\n",
              "      <th>sales_lag_6</th>\n",
              "      <th>sales_lag_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_014_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_056_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_062_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333077</th>\n",
              "      <td>FOODS_3_807_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333078</th>\n",
              "      <td>FOODS_3_810_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333079</th>\n",
              "      <td>FOODS_3_814_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333080</th>\n",
              "      <td>FOODS_3_815_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333081</th>\n",
              "      <td>FOODS_3_817_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2333082 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    id       d  ...  sales_lag_6  sales_lag_7\n",
              "0        HOBBIES_1_010_CA_1_validation     d_1  ...          NaN          NaN\n",
              "1        HOBBIES_1_012_CA_1_validation     d_1  ...          NaN          NaN\n",
              "2        HOBBIES_1_014_CA_1_validation     d_1  ...          NaN          NaN\n",
              "3        HOBBIES_1_056_CA_1_validation     d_1  ...          NaN          NaN\n",
              "4        HOBBIES_1_062_CA_1_validation     d_1  ...          NaN          NaN\n",
              "...                                ...     ...  ...          ...          ...\n",
              "2333077    FOODS_3_807_CA_4_validation  d_1941  ...          4.0          0.0\n",
              "2333078    FOODS_3_810_CA_4_validation  d_1941  ...          2.0          5.0\n",
              "2333079    FOODS_3_814_CA_4_validation  d_1941  ...          1.0          1.0\n",
              "2333080    FOODS_3_815_CA_4_validation  d_1941  ...          0.0          0.0\n",
              "2333081    FOODS_3_817_CA_4_validation  d_1941  ...          0.0          0.0\n",
              "\n",
              "[2333082 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEcjnVxl9HMC"
      },
      "source": [
        "***We can notice many NaNs values - which is normal because there is no data for day 0,-1,-2 (out of dataset time periods)\n",
        "\n",
        "***Same works for test set*** \n",
        "\n",
        "**be careful to make lag features:**\n",
        "- *for day 1920 there is no data about day 1919 (until 1913)\n",
        "- *So if we want to predict day 1915, our lag features have to start from 2 (1915 (which is the predicting day)-1913 (the last day with label in dataset), and so on*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3gHwqpAkcKj",
        "outputId": "2a740af6-c3c3-4d0f-8135-3793f1a61b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "# The result\n",
        "temp_df[temp_df['id']=='HOBBIES_1_010_CA_1_validation'].iloc[:10]\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>sales_lag_1</th>\n",
              "      <th>sales_lag_2</th>\n",
              "      <th>sales_lag_3</th>\n",
              "      <th>sales_lag_4</th>\n",
              "      <th>sales_lag_5</th>\n",
              "      <th>sales_lag_6</th>\n",
              "      <th>sales_lag_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3606</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4808</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6010</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_6</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7212</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_7</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_8</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9616</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10818</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id     d  ...  sales_lag_6  sales_lag_7\n",
              "0      HOBBIES_1_010_CA_1_validation   d_1  ...          NaN          NaN\n",
              "1202   HOBBIES_1_010_CA_1_validation   d_2  ...          NaN          NaN\n",
              "2404   HOBBIES_1_010_CA_1_validation   d_3  ...          NaN          NaN\n",
              "3606   HOBBIES_1_010_CA_1_validation   d_4  ...          NaN          NaN\n",
              "4808   HOBBIES_1_010_CA_1_validation   d_5  ...          NaN          NaN\n",
              "6010   HOBBIES_1_010_CA_1_validation   d_6  ...          NaN          NaN\n",
              "7212   HOBBIES_1_010_CA_1_validation   d_7  ...          0.0          NaN\n",
              "8414   HOBBIES_1_010_CA_1_validation   d_8  ...          0.0          0.0\n",
              "9616   HOBBIES_1_010_CA_1_validation   d_9  ...          1.0          0.0\n",
              "10818  HOBBIES_1_010_CA_1_validation  d_10  ...          0.0          1.0\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWD15xcp-ZKE"
      },
      "source": [
        "***Rolling lags***\n",
        "\n",
        "- *Because of computational limitation, we restored few days of sales values from horizontal representation as lag features* \n",
        "\n",
        "- *lambda x: x.shift(1)==> 1 day shift will serve only to predict day 1914\n",
        "\n",
        "- *for other days we have to shift PREDICT_DAY-1913*\n",
        "\n",
        "- *Lets go for rolling aggregation* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jom6dbmAm2wt",
        "outputId": "5dc3212c-f9bc-4454-a6e3-15606cf5cfb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "########################### Rolling lags ###########################################################\n",
        "\n",
        "## Rolling aggregations\n",
        "\n",
        "\n",
        "\n",
        "# Such aggregations will help us to restore\n",
        "# at least part of the information for our model\n",
        "\n",
        "temp_df = train_df[['id','d','sales']]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i in [14,30,60]:\n",
        "    print('Rolling period:', i)\n",
        "    temp_df['rolling_mean_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
        "    temp_df['rolling_std_'+str(i)]  = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
        "\n",
        "# we can also aggregate by max/skew/median etc \n",
        "\n",
        "print('%0.2f min: Time for loop' % ((time.time() - start_time) / 60))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rolling period: 14\n",
            "Rolling period: 30\n",
            "Rolling period: 60\n",
            "0.19 min: Time for loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsSNqT99B8lb",
        "outputId": "80806bcf-561b-430a-89f1-21175d13b15a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "104/6"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.333333333333332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gEvUX6kcUP",
        "outputId": "3b1083cb-5fcd-444d-8248-d1cfa1a6613c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "# The result \n",
        "temp_df[temp_df['id']=='HOBBIES_1_012_CA_1_validation'].iloc[:20]\n",
        "\n",
        "# Same as before, the NaNs values - is is because of absence of data for \n",
        "# 0*(rolling_period),-1*(rolling_period),-2*(rolling_period)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>rolling_mean_14</th>\n",
              "      <th>rolling_std_14</th>\n",
              "      <th>rolling_mean_30</th>\n",
              "      <th>rolling_std_30</th>\n",
              "      <th>rolling_mean_60</th>\n",
              "      <th>rolling_std_60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_3</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3607</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_4</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4809</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6011</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_6</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7213</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_7</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8415</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_8</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9617</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_9</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10819</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_10</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12021</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_11</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13223</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_12</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14425</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_13</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15627</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_14</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16829</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_15</td>\n",
              "      <td>0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18031</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_16</td>\n",
              "      <td>2</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19233</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_17</td>\n",
              "      <td>0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20435</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_18</td>\n",
              "      <td>3</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21637</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_19</td>\n",
              "      <td>0</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>1.081818</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22839</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_20</td>\n",
              "      <td>1</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>1.081818</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id     d  ...  rolling_mean_60  rolling_std_60\n",
              "1      HOBBIES_1_012_CA_1_validation   d_1  ...              NaN             NaN\n",
              "1203   HOBBIES_1_012_CA_1_validation   d_2  ...              NaN             NaN\n",
              "2405   HOBBIES_1_012_CA_1_validation   d_3  ...              NaN             NaN\n",
              "3607   HOBBIES_1_012_CA_1_validation   d_4  ...              NaN             NaN\n",
              "4809   HOBBIES_1_012_CA_1_validation   d_5  ...              NaN             NaN\n",
              "6011   HOBBIES_1_012_CA_1_validation   d_6  ...              NaN             NaN\n",
              "7213   HOBBIES_1_012_CA_1_validation   d_7  ...              NaN             NaN\n",
              "8415   HOBBIES_1_012_CA_1_validation   d_8  ...              NaN             NaN\n",
              "9617   HOBBIES_1_012_CA_1_validation   d_9  ...              NaN             NaN\n",
              "10819  HOBBIES_1_012_CA_1_validation  d_10  ...              NaN             NaN\n",
              "12021  HOBBIES_1_012_CA_1_validation  d_11  ...              NaN             NaN\n",
              "13223  HOBBIES_1_012_CA_1_validation  d_12  ...              NaN             NaN\n",
              "14425  HOBBIES_1_012_CA_1_validation  d_13  ...              NaN             NaN\n",
              "15627  HOBBIES_1_012_CA_1_validation  d_14  ...              NaN             NaN\n",
              "16829  HOBBIES_1_012_CA_1_validation  d_15  ...              NaN             NaN\n",
              "18031  HOBBIES_1_012_CA_1_validation  d_16  ...              NaN             NaN\n",
              "19233  HOBBIES_1_012_CA_1_validation  d_17  ...              NaN             NaN\n",
              "20435  HOBBIES_1_012_CA_1_validation  d_18  ...              NaN             NaN\n",
              "21637  HOBBIES_1_012_CA_1_validation  d_19  ...              NaN             NaN\n",
              "22839  HOBBIES_1_012_CA_1_validation  d_20  ...              NaN             NaN\n",
              "\n",
              "[20 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdfoVdyRDBPe"
      },
      "source": [
        "***'Memory usage' minify mechanism***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6huSsNzkcgf",
        "outputId": "531831a7-3327-4481-f402-80bc4655897b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "########################### Memory ussage ###################################################\n",
        "# Let's check our memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Original rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n",
        "\n",
        "# can we minify it?\n",
        "# 1. if our dataset are aligned by index \n",
        "#    we don't need 'id' 'd' 'sales' columns\n",
        "temp_df = temp_df.iloc[:,3:]\n",
        "print(\"{:>20}: {:>8}\".format('Values rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n",
        "\n",
        "# can we make it even smaller?\n",
        "# yes, carefully change dtype and/or use sparce matrix to minify 0s\n",
        "# Also note that lgbm accepts matrixes as input that is good for memory reducion \n",
        "from scipy import sparse \n",
        "temp_matrix = sparse.csr_matrix(temp_df)\n",
        "\n",
        "# restore to df\n",
        "temp_matrix_restored = pd.DataFrame(temp_matrix.todense())\n",
        "restored_cols = ['roll_' + str(i) for i in list(temp_matrix_restored)]\n",
        "temp_matrix_restored.columns = restored_cols"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original rolling df: 160.2MiB\n",
            "   Values rolling df: 106.8MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Peht1m7Akctn"
      },
      "source": [
        "########################### Remove old objects\n",
        "#################################################################################\n",
        "del temp_df, train_df, temp_matrix, temp_matrix_restored"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJgyobYGkc-2",
        "outputId": "e950fdab-e7b1-41dd-baed-d244a77ff153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "########################### Apply on grid_df #####################################################\n",
        "# lets read first grid from \n",
        "grid_df = pd.read_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_1.pkl')\n",
        "\n",
        "# We need only 'id','d','sales'\n",
        "# to make lags and rollings\n",
        "grid_df = grid_df[['id','d','sales']]\n",
        "SHIFT_DAY = 28\n",
        "\n",
        "# Lags\n",
        "# with 28 day shift\n",
        "start_time = time.time()\n",
        "print('Create lags')\n",
        "\n",
        "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n",
        "grid_df = grid_df.assign(**{\n",
        "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
        "        for l in LAG_DAYS\n",
        "        for col in [TARGET]\n",
        "    })\n",
        "\n",
        "# Minify lag columns\n",
        "for col in list(grid_df):\n",
        "    if 'lag' in col:\n",
        "        grid_df[col] = grid_df[col].astype(np.float16)\n",
        "\n",
        "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
        "\n",
        "# Rollings\n",
        "# with 28 day shift\n",
        "start_time = time.time()\n",
        "print('Create rolling aggs')\n",
        "\n",
        "for i in [7,14,30,60,180]:\n",
        "    print('Rolling period:', i)\n",
        "    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n",
        "    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n",
        "\n",
        "# Rollings\n",
        "# with sliding shift\n",
        "for d_shift in [1,7,14]: \n",
        "    print('Shifting period:', d_shift)\n",
        "    for d_window in [7,14,30,60]:\n",
        "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
        "        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
        "    \n",
        "    \n",
        "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create lags\n",
            "0.51 min: Lags\n",
            "Create rolling aggs\n",
            "Rolling period: 7\n",
            "Rolling period: 14\n",
            "Rolling period: 30\n",
            "Rolling period: 60\n",
            "Rolling period: 180\n",
            "Shifting period: 1\n",
            "Shifting period: 7\n",
            "Shifting period: 14\n",
            "1.18 min: Lags\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4kn4m65kdSY",
        "outputId": "4a3e0275-b53f-404e-dd42-1bc81235824b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "########################### Export ###############################################\n",
        "path='/content/drive/My Drive/GoogleColab/pickle_files/'\n",
        "print('Save lags and rollings')\n",
        "grid_df.to_pickle(path+'lags_df_'+str(SHIFT_DAY)+'.pkl')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save lags and rollings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNG-jEAOkdwu",
        "outputId": "0eeecdb8-3a1a-4017-8e78-f0c57d753af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "########################### Final list of new features ################################################\n",
        "grid_df.info()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 40 columns):\n",
            " #   Column                  Dtype   \n",
            "---  ------                  -----   \n",
            " 0   id                      category\n",
            " 1   d                       int16   \n",
            " 2   sales                   float64 \n",
            " 3   sales_lag_28            float16 \n",
            " 4   sales_lag_29            float16 \n",
            " 5   sales_lag_30            float16 \n",
            " 6   sales_lag_31            float16 \n",
            " 7   sales_lag_32            float16 \n",
            " 8   sales_lag_33            float16 \n",
            " 9   sales_lag_34            float16 \n",
            " 10  sales_lag_35            float16 \n",
            " 11  sales_lag_36            float16 \n",
            " 12  sales_lag_37            float16 \n",
            " 13  sales_lag_38            float16 \n",
            " 14  sales_lag_39            float16 \n",
            " 15  sales_lag_40            float16 \n",
            " 16  sales_lag_41            float16 \n",
            " 17  sales_lag_42            float16 \n",
            " 18  rolling_mean_7          float16 \n",
            " 19  rolling_std_7           float16 \n",
            " 20  rolling_mean_14         float16 \n",
            " 21  rolling_std_14          float16 \n",
            " 22  rolling_mean_30         float16 \n",
            " 23  rolling_std_30          float16 \n",
            " 24  rolling_mean_60         float16 \n",
            " 25  rolling_std_60          float16 \n",
            " 26  rolling_mean_180        float16 \n",
            " 27  rolling_std_180         float16 \n",
            " 28  rolling_mean_tmp_1_7    float16 \n",
            " 29  rolling_mean_tmp_1_14   float16 \n",
            " 30  rolling_mean_tmp_1_30   float16 \n",
            " 31  rolling_mean_tmp_1_60   float16 \n",
            " 32  rolling_mean_tmp_7_7    float16 \n",
            " 33  rolling_mean_tmp_7_14   float16 \n",
            " 34  rolling_mean_tmp_7_30   float16 \n",
            " 35  rolling_mean_tmp_7_60   float16 \n",
            " 36  rolling_mean_tmp_14_7   float16 \n",
            " 37  rolling_mean_tmp_14_14  float16 \n",
            " 38  rolling_mean_tmp_14_30  float16 \n",
            " 39  rolling_mean_tmp_14_60  float16 \n",
            "dtypes: category(1), float16(37), float64(1), int16(1)\n",
            "memory usage: 375.6 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXK0VVvcopXy"
      },
      "source": [
        "# Part 3: Mean encoding (Custom encoding)\n",
        "- 1. FE creation approaches\n",
        "- 2. Sequential FE validation\n",
        "- 3. Dimension reduction\n",
        "- 4. FE validation by Permutation importance\n",
        "- 5. Mean encodings\n",
        "- 6. Parallelization for FE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0chNXY9kdqs"
      },
      "source": [
        "#read data\n",
        "#Lets read same 3 sets of features\n",
        "path='/content/drive/My Drive/GoogleColab/pickle_files/'\n",
        "grid_df = pd.concat([pd.read_pickle(path+'grid_part_1.pkl'),\n",
        "                     pd.read_pickle(path+'grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(path+'grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        " ## OR\n",
        "#grid_df=pd.concat([grid_part_1, grid_part_2.iloc[:,2:], grid_part_3.iloc[:,2:]], axis=1)\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKB7tCiki-Dd",
        "outputId": "9561640d-7719-4e15-f07d-ca34591a8337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 25 columns):\n",
            " #   Column        Dtype   \n",
            "---  ------        -----   \n",
            " 0   id            category\n",
            " 1   item_id       category\n",
            " 2   dept_id       category\n",
            " 3   cat_id        category\n",
            " 4   store_id      category\n",
            " 5   state_id      category\n",
            " 6   d             int16   \n",
            " 7   sales         float64 \n",
            " 8   release       int16   \n",
            " 9   wm_yr_wk      int64   \n",
            " 10  sell_price    float16 \n",
            " 11  event_name_1  category\n",
            " 12  event_type_1  category\n",
            " 13  event_name_2  category\n",
            " 14  event_type_2  category\n",
            " 15  snap_CA       category\n",
            " 16  snap_TX       category\n",
            " 17  snap_WI       category\n",
            " 18  tm_d          int8    \n",
            " 19  tm_w          int8    \n",
            " 20  tm_m          int8    \n",
            " 21  tm_y          int8    \n",
            " 22  tm_wm         int8    \n",
            " 23  tm_dw         int8    \n",
            " 24  tm_w_end      int8    \n",
            "dtypes: category(13), float16(1), float64(1), int16(2), int64(1), int8(7)\n",
            "memory usage: 192.3 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks41QLnFkdkx",
        "outputId": "d9dc0fae-6ce8-4888-d6ba-f931c85a09ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# Subsampling\n",
        "# to make all calculations faster.\n",
        "# Keep only 5% of original ids.\n",
        "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
        "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
        "\n",
        "# Let's \"inspect\" our grid DataFrame\n",
        "grid_df.info()\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 300865 entries, 0 to 300864\n",
            "Data columns (total 25 columns):\n",
            " #   Column        Non-Null Count   Dtype   \n",
            "---  ------        --------------   -----   \n",
            " 0   id            300865 non-null  category\n",
            " 1   item_id       300865 non-null  category\n",
            " 2   dept_id       300865 non-null  category\n",
            " 3   cat_id        300865 non-null  category\n",
            " 4   store_id      300865 non-null  category\n",
            " 5   state_id      300865 non-null  category\n",
            " 6   d             300865 non-null  int16   \n",
            " 7   sales         296581 non-null  float64 \n",
            " 8   release       300865 non-null  int16   \n",
            " 9   wm_yr_wk      300865 non-null  int64   \n",
            " 10  sell_price    30956 non-null   float16 \n",
            " 11  event_name_1  24786 non-null   category\n",
            " 12  event_type_1  24786 non-null   category\n",
            " 13  event_name_2  612 non-null     category\n",
            " 14  event_type_2  612 non-null     category\n",
            " 15  snap_CA       300865 non-null  category\n",
            " 16  snap_TX       300865 non-null  category\n",
            " 17  snap_WI       300865 non-null  category\n",
            " 18  tm_d          300865 non-null  int8    \n",
            " 19  tm_w          300865 non-null  int8    \n",
            " 20  tm_m          300865 non-null  int8    \n",
            " 21  tm_y          300865 non-null  int8    \n",
            " 22  tm_wm         300865 non-null  int8    \n",
            " 23  tm_dw         300865 non-null  int8    \n",
            " 24  tm_w_end      300865 non-null  int8    \n",
            "dtypes: category(13), float16(1), float64(1), int16(2), int64(1), int8(7)\n",
            "memory usage: 12.8 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eev6BLp3hwh9",
        "outputId": "6c616064-826f-4e68-a250-60a752060573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "grid_df.head(2)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>release</th>\n",
              "      <th>wm_yr_wk</th>\n",
              "      <th>sell_price</th>\n",
              "      <th>event_name_1</th>\n",
              "      <th>event_type_1</th>\n",
              "      <th>event_name_2</th>\n",
              "      <th>event_type_2</th>\n",
              "      <th>snap_CA</th>\n",
              "      <th>snap_TX</th>\n",
              "      <th>snap_WI</th>\n",
              "      <th>tm_d</th>\n",
              "      <th>tm_w</th>\n",
              "      <th>tm_m</th>\n",
              "      <th>tm_y</th>\n",
              "      <th>tm_wm</th>\n",
              "      <th>tm_dw</th>\n",
              "      <th>tm_w_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_303_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_303</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>11101</td>\n",
              "      <td>8.976562</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOUSEHOLD_1_401_CA_1_validation</td>\n",
              "      <td>HOUSEHOLD_1_401</td>\n",
              "      <td>HOUSEHOLD_1</td>\n",
              "      <td>HOUSEHOLD</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>11101</td>\n",
              "      <td>8.867188</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                id          item_id  ... tm_dw tm_w_end\n",
              "0    HOBBIES_1_303_CA_1_validation    HOBBIES_1_303  ...     5        1\n",
              "1  HOUSEHOLD_1_401_CA_1_validation  HOUSEHOLD_1_401  ...     5        1\n",
              "\n",
              "[2 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0S6k1w2dHnj"
      },
      "source": [
        "# Part 3.1 Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRz6_oOYkdQ0"
      },
      "source": [
        "########################### Baseline model ##########################################\n",
        "\n",
        "# We will need some global VARS for future\n",
        "\n",
        "SEED = 142             # Our random seed for everything\n",
        "random.seed(SEED)      # to make all tests \"deterministic\"\n",
        "np.random.seed(SEED)\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores (we have just 2)\n",
        "\n",
        "TARGET = 'sales'      # Our Target\n",
        "END_TRAIN = 1913      # And we will use last 28 days as validation\n",
        "\n",
        "# Drop some items from \"TEST\" set part (1914...)\n",
        "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
        "\n",
        "# Features that we want to exclude from training\n",
        "remove_features = ['id','d',TARGET]\n",
        "\n",
        "# Our baseline model serves to do fast checks of new features performance \n",
        "\n",
        "# We will use LightGBM for our tests\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
        "                    'objective': 'regression',       # Standart loss for RMSE\n",
        "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
        "                    'subsample': 0.8,                \n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
        "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
        "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
        "                    'feature_fraction': 0.8,\n",
        "                    'n_estimators': 5000,            # We don't want to limit training (we can change 5000 to any big enough number)\n",
        "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
        "                    'seed': SEED,\n",
        "                    'verbose': -1,\n",
        "                } \n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oActm3NYkdPA",
        "outputId": "c9361ae1-bd19-484e-8257-9a1a83c97a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "## RMSE\n",
        "def rmse(y, y_pred):\n",
        "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
        "\n",
        "# Small function to make fast features tests\n",
        "# estimator = make_fast_test(grid_df)\n",
        "# it will return lgb booster for future analisys\n",
        "def make_fast_test(df):\n",
        "\n",
        "    features_columns = [col for col in list(df) if col not in remove_features]\n",
        "\n",
        "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n",
        "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
        "    \n",
        "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
        "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
        "    \n",
        "    estimator = lgb.train(\n",
        "                            lgb_params,\n",
        "                            train_data,\n",
        "                            valid_sets = [train_data,valid_data],\n",
        "                            verbose_eval = 500,\n",
        "                        )\n",
        "    \n",
        "    return estimator\n",
        "\n",
        "# Make baseline model\n",
        "baseline_model = make_fast_test(grid_df)\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[307]\ttraining's rmse: 1.88327\tvalid_1's rmse: 1.85104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0JoRVH4kc6g",
        "outputId": "86b18175-0f93-48d2-f34f-427488812d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Lets test our normal Lags (7 days)###########################\n",
        "\n",
        "# Small helper to make lags creation faster\n",
        "from multiprocessing import Pool                # Multiprocess Runs\n",
        "\n",
        "## Multiprocessing Run.\n",
        "# :t_split - int of lags days                   # type: int\n",
        "# :func - Function to apply on each split       # type: python function\n",
        "\n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "def make_normal_lag(lag_day):\n",
        "    lag_df = grid_df[['id','d',TARGET]] \n",
        "    col_name = 'sales_lag_'+str(lag_day)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "# Launch parallel lag creation\n",
        "# and \"append\" to our grid\n",
        "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[179]\ttraining's rmse: 1.79122\tvalid_1's rmse: 1.76335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amgkFuv1kc3P",
        "outputId": "69189973-33d7-4e81-b004-32b81a743811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "########################### Permutation importance Test\n",
        "########################### https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "#################################################################################\n",
        "\n",
        "# Let's creat validation dataset and features\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "\n",
        "# Make normal prediction with our model and save score\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "\n",
        "# Now we are looping over all our numerical features\n",
        "for col in features_columns:\n",
        "    \n",
        "    # We will make validation set copy to restore\n",
        "    # features states on each run\n",
        "    temp_df = validation_df.copy()\n",
        "    \n",
        "    # Error here appears if we have \"categorical\" features and can't \n",
        "    # do np.random.permutation without disrupt categories\n",
        "    # so we need to check if feature is numerical\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        \n",
        "        # If our current rmse score is less than base score\n",
        "        # it means that feature most probably is a bad one\n",
        "        # and our model is learning on noise\n",
        "        print(col, np.round(cur_score - base_score, 4))\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standart RMSE 1.7633459853087066\n",
            "release -0.0001\n",
            "wm_yr_wk 0.0\n",
            "sell_price 0.0\n",
            "tm_d 0.0124\n",
            "tm_w -0.0001\n",
            "tm_m -0.0001\n",
            "tm_y 0.0\n",
            "tm_wm 0.0004\n",
            "tm_dw 0.1121\n",
            "tm_w_end 0.0061\n",
            "sales_lag_1 0.2887\n",
            "sales_lag_2 0.0363\n",
            "sales_lag_3 0.0426\n",
            "sales_lag_4 0.0393\n",
            "sales_lag_5 0.0077\n",
            "sales_lag_6 0.012\n",
            "sales_lag_7 0.0451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQtTOnVVAgp"
      },
      "source": [
        "# Insights:\n",
        "- Lags with 1 days shift (nearest past) shows the important features while other features not much. \n",
        "\n",
        "- Better to test with several Permutation for confirmation\n",
        "(https://www.kaggle.com/dansbecker/permutation-importance @dansbecker)\n",
        "\n",
        "- price_nunique -0.002 : strong negative values are most probably noise\n",
        "\n",
        "- The idea is the following: feature importance can be measured by looking at how much the score (accuracy, mse, rmse, mae, etc. - any score we’re interested in) decreases when a feature is not available.\n",
        "\n",
        "- To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n",
        "\n",
        "- To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. \n",
        "\n",
        "- Instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. \n",
        "\n",
        "- This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Zsyio1kc0z"
      },
      "source": [
        "# Remove Temp data\n",
        "del temp_df, validation_df\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qre4J75TkcmD",
        "outputId": "bccdb1ff-7607-4a33-d0b0-6411d0c1a31f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "\n",
        "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "for col in features_columns:\n",
        "    temp_df = validation_df.copy()\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "del temp_df, validation_df"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[219]\ttraining's rmse: 1.93686\tvalid_1's rmse: 1.86004\n",
            "Standart RMSE 1.8600396804547472\n",
            "release 0.0001\n",
            "wm_yr_wk 0.0\n",
            "sell_price 0.0009\n",
            "tm_d 0.0069\n",
            "tm_w 0.0025\n",
            "tm_m -0.002\n",
            "tm_y 0.0\n",
            "tm_wm 0.0011\n",
            "tm_dw 0.0536\n",
            "tm_w_end 0.0046\n",
            "sales_lag_56 0.0461\n",
            "sales_lag_57 0.0268\n",
            "sales_lag_58 0.0071\n",
            "sales_lag_59 0.0157\n",
            "sales_lag_60 0.0087\n",
            "sales_lag_61 0.0141\n",
            "sales_lag_62 0.0157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgS0RrRskcd8"
      },
      "source": [
        "     \n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 56 days shift (far away past) are not as important\n",
        "## as nearest past lags\n",
        "## and at some point will be just noise for our model"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtvZPGWLdTs9"
      },
      "source": [
        "# Part 3.2 Implementation of PCA algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChrPUkEhkca3"
      },
      "source": [
        "########################### PCA #################################################\n",
        "\n",
        "# The main question here - can we have almost same rmse boost with less features\n",
        "# less dimensionality?\n",
        "\n",
        "# Lets try PCA and make 7->3 dimensionality reduction\n",
        "\n",
        "# PCA is \"unsupervised\" learning and with shifted target we can be sure\n",
        "# that we have no Target leakage\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def make_pca(df, pca_col, n_days):\n",
        "    print('PCA:', pca_col, n_days)\n",
        "    \n",
        "    # We don't need any other columns to make pca\n",
        "    pca_df = df[[pca_col,'d',TARGET]]\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\" \n",
        "    # we need to agg first\n",
        "    if pca_col != 'id':\n",
        "        merge_base = pca_df[[pca_col,'d']]\n",
        "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
        "        pca_df[TARGET] = pca_df['sum']\n",
        "        del pca_df['sum']\n",
        "    \n",
        "    # Min/Max scaling\n",
        "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
        "    \n",
        "    # Making \"lag\" in old way (not parallel)\n",
        "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
        "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
        "    pca_df = pca_df.assign(**{\n",
        "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
        "            for l in LAG_DAYS\n",
        "            for col in [TARGET]\n",
        "        })\n",
        "    \n",
        "    pca_columns = list(pca_df)[3:]\n",
        "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
        "    pca = PCA(random_state=SEED)\n",
        "    \n",
        "    # You can use fit_transform here\n",
        "    pca.fit(pca_df[pca_columns])\n",
        "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
        "    \n",
        "    print(pca.explained_variance_ratio_)\n",
        "    \n",
        "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
        "    keep_cols = pca_columns[:3]\n",
        "    print('Columns to keep:', keep_cols)\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\"\n",
        "    # we need merge back our results to merge_base df\n",
        "    # and only than return resulted df\n",
        "    # I'll skip that step here\n",
        "    \n",
        "    return pca_df[keep_cols]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTOVRFfskcRT",
        "outputId": "f980c43f-9b8b-4ecb-a970-c82edc0ddeb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "\n",
        "# Make PCA\n",
        "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA: id 7\n",
            "[0.71783945 0.06713417 0.05399105 0.04415797 0.04118687 0.03797171\n",
            " 0.03771877]\n",
            "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[240]\ttraining's rmse: 1.79727\tvalid_1's rmse: 1.77568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19egoP1pq0j-",
        "outputId": "f2a912d2-bd67-4326-c9f6-b12e03becae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "########################### Mean/std target encoding ################################################\n",
        "\n",
        "# We will use these three columns for test\n",
        "# (in combination with store_id)\n",
        "icols = ['item_id','cat_id','dept_id']\n",
        "\n",
        "# But we can use any other column or even multiple groups\n",
        "# like these ones\n",
        "#            'state_id',\n",
        "#            'store_id',\n",
        "#            'cat_id',\n",
        "#            'dept_id',\n",
        "#            ['state_id', 'cat_id'],\n",
        "#            ['state_id', 'dept_id'],\n",
        "#            ['store_id', 'cat_id'],\n",
        "#            ['store_id', 'dept_id'],\n",
        "#            'item_id',\n",
        "#            ['item_id', 'state_id'],\n",
        "#            ['item_id', 'store_id']\n",
        "\n",
        "# There are several ways to do \"mean\" encoding\n",
        "## K-fold scheme\n",
        "## LOO (leave one out)\n",
        "## Smoothed/regularized \n",
        "## Expanding mean\n",
        "## etc \n",
        "\n",
        "# We will use simple target encoding\n",
        "# by std and mean agg\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n",
        "    \n",
        "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
        "    joiner = '_'+col+'_encoding_'\n",
        "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
        "    temp_df = temp_df.reset_index()\n",
        "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
        "    del temp_df\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding item_id\n",
            "Encoding cat_id\n",
            "Encoding dept_id\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EFJ3SoNq6DA",
        "outputId": "76e051bf-617e-4e3a-f742-1075363aee96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "# Bad thing that for some items  \n",
        "# we are using past and future values.\n",
        "# But we are looking for \"categorical\" similiarity\n",
        "# on a \"long run\". So future here is not a big problem.\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[271]\ttraining's rmse: 1.8878\tvalid_1's rmse: 1.85755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHKR5eMLrInC"
      },
      "source": [
        "\n",
        "########################### Last Non 0 sale ##########################\n",
        "\n",
        "def find_last_sale(df,n_day):\n",
        "    \n",
        "    # Limit initial df\n",
        "    ls_df = df[['id','d',TARGET]]\n",
        "    \n",
        "    # Convert target to binary\n",
        "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
        "    \n",
        "    # Make lags to prevent any leakage\n",
        "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
        "\n",
        "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
        "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
        "\n",
        "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
        "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
        "\n",
        "    return ls_df[['last_sale']]\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu0pWzYarJSc",
        "outputId": "40021220-52d6-4a79-9938-6023750a5636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "# Find last non zero\n",
        "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "########################### Apply on grid_df ###################################################\n",
        "# lets read grid from \n",
        "grid_df = pd.read_pickle(path+'grid_part_1.pkl')\n",
        "#grid_df=grid_part_1.copy()\n",
        "#grid_df['d']=grid_df['d'].str[2:6].astype(int) #need to change 'd' into numeric\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[370]\ttraining's rmse: 1.79999\tvalid_1's rmse: 1.78604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOiQffuorJgK",
        "outputId": "a37a9304-0034-473e-ad36-c92a3b755717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "\n",
        "icols =  [\n",
        "            ['state_id'],\n",
        "            ['store_id'],\n",
        "            ['cat_id'],\n",
        "            ['dept_id'],\n",
        "            ['state_id', 'cat_id'],\n",
        "            ['state_id', 'dept_id'],\n",
        "            ['store_id', 'cat_id'],\n",
        "            ['store_id', 'dept_id'],\n",
        "            ['item_id'],\n",
        "            ['item_id', 'state_id'],\n",
        "            ['item_id', 'store_id']\n",
        "            ]\n",
        "\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    col_name = '_'+'_'.join(col)+'_'\n",
        "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
        "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]\n",
        "\n",
        "# #################################################################################\n",
        "print('Save Mean/Std encoding')\n",
        "grid_df.to_pickle(path+'mean_encoding_df.pkl')\n",
        "\n",
        "########################### Final list of new features\n",
        "#################################################################################\n",
        "grid_df.info()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding ['state_id']\n",
            "Encoding ['store_id']\n",
            "Encoding ['cat_id']\n",
            "Encoding ['dept_id']\n",
            "Encoding ['state_id', 'cat_id']\n",
            "Encoding ['state_id', 'dept_id']\n",
            "Encoding ['store_id', 'cat_id']\n",
            "Encoding ['store_id', 'dept_id']\n",
            "Encoding ['item_id']\n",
            "Encoding ['item_id', 'state_id']\n",
            "Encoding ['item_id', 'store_id']\n",
            "Save Mean/Std encoding\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 24 columns):\n",
            " #   Column                     Dtype   \n",
            "---  ------                     -----   \n",
            " 0   id                         category\n",
            " 1   d                          int16   \n",
            " 2   enc_state_id_mean          float16 \n",
            " 3   enc_state_id_std           float16 \n",
            " 4   enc_store_id_mean          float16 \n",
            " 5   enc_store_id_std           float16 \n",
            " 6   enc_cat_id_mean            float16 \n",
            " 7   enc_cat_id_std             float16 \n",
            " 8   enc_dept_id_mean           float16 \n",
            " 9   enc_dept_id_std            float16 \n",
            " 10  enc_state_id_cat_id_mean   float16 \n",
            " 11  enc_state_id_cat_id_std    float16 \n",
            " 12  enc_state_id_dept_id_mean  float16 \n",
            " 13  enc_state_id_dept_id_std   float16 \n",
            " 14  enc_store_id_cat_id_mean   float16 \n",
            " 15  enc_store_id_cat_id_std    float16 \n",
            " 16  enc_store_id_dept_id_mean  float16 \n",
            " 17  enc_store_id_dept_id_std   float16 \n",
            " 18  enc_item_id_mean           float16 \n",
            " 19  enc_item_id_std            float16 \n",
            " 20  enc_item_id_state_id_mean  float16 \n",
            " 21  enc_item_id_state_id_std   float16 \n",
            " 22  enc_item_id_store_id_mean  float16 \n",
            " 23  enc_item_id_store_id_std   float16 \n",
            "dtypes: category(1), float16(22), int16(1)\n",
            "memory usage: 209.6 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR86roSDr1ix"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meJYQBqJix1g"
      },
      "source": [
        "# Part 4 Final forecasting\n",
        "- part 4.1 Parallelization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncMjtqQ9i42D"
      },
      "source": [
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnfIlWCHekx5"
      },
      "source": [
        "# Part 4.2 get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xvz3ombi9H8"
      },
      "source": [
        "def get_data_by_store(store):\n",
        "    df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)\n",
        "\n",
        "    df = df[df['store_id']==store]\n",
        "    \n",
        "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
        "    df2 = df2[df2.index.isin(df.index)]\n",
        "    \n",
        "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
        "    df3 = df3[df3.index.isin(df.index)]\n",
        "    \n",
        "    df = pd.concat([df, df2], axis=1)\n",
        "    del df2 \n",
        "    \n",
        "    df = pd.concat([df, df3], axis=1)\n",
        "    del df3 \n",
        "    features = [col for col in list(df) if col not in remove_features]\n",
        "    df = df[['id','d',TARGET]+features]\n",
        "    \n",
        "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
        "    \n",
        "    return df, features\n",
        "\n",
        "def get_base_test():\n",
        "    base_test = pd.DataFrame()\n",
        "\n",
        "    for store_id in STORES_IDS:\n",
        "        temp_df = pd.read_pickle('/content/drive/My Drive/GoogleColab/AUX_MODELS/test_'+store_id+'.pkl')\n",
        "        temp_df['store_id'] = store_id\n",
        "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
        "    \n",
        "    return base_test\n",
        "\n",
        "\n",
        "def make_lag(LAG_DAY):\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "\n",
        "def make_lag_roll(LAG_DAY):\n",
        "    shift_day = LAG_DAY[0]\n",
        "    roll_wind = LAG_DAY[1]\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
        "    return lag_df[[col_name]]\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGjDH6YFe4ny"
      },
      "source": [
        "# Part 4.3 Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELyBgLeejLOE"
      },
      "source": [
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',\n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.1,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 1400,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': -1,\n",
        "                }"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNHum1FLe-iS"
      },
      "source": [
        "# Part 4.4 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F422iYSWjRCR"
      },
      "source": [
        "VER = 1                          # Our model version\n",
        "SEED = 42                        # We want all things\n",
        "seed_everything(SEED)            # to be as deterministic \n",
        "lgb_params['seed'] = SEED        # as possible\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "\n",
        "#LIMITS and const\n",
        "TARGET      = 'sales'            # Our target\n",
        "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
        "END_TRAIN   = 1913               # End day of our train set\n",
        "P_HORIZON   = 28                 # Prediction horizon\n",
        "USE_AUX     = True               # Use or not pretrained models\n",
        "\n",
        "\n",
        "remove_features = ['id','state_id','store_id',\n",
        "                   'date','wm_yr_wk','d',TARGET]\n",
        "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
        "                   'enc_dept_id_mean','enc_dept_id_std',\n",
        "                   'enc_item_id_mean','enc_item_id_std'] \n",
        "\n",
        "# #PATHS for Features\n",
        "# ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
        "# BASE     = '../input/m5-simple-fe/grid_part_1.pkl'\n",
        "# PRICE    = '../input/m5-simple-fe/grid_part_2.pkl'\n",
        "# CALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\n",
        "# LAGS     = '../input/m5-lags-features/lags_df_28.pkl'\n",
        "# MEAN_ENC = '../input/m5-custom-features/mean_encoding_df.pkl'\n",
        "\n",
        "#PATHS for Features\n",
        "ORIGINAL = '/content/drive/My Drive/GoogleColab/'\n",
        "BASE     = path+'grid_part_1.pkl'\n",
        "PRICE    = path+'grid_part_2.pkl'\n",
        "CALENDAR = path+'grid_part_3.pkl'\n",
        "LAGS     = path+'lags_df_28.pkl'\n",
        "MEAN_ENC = path+'mean_encoding_df.pkl'\n",
        "\n",
        "# AUX(pretrained) Models paths\n",
        "AUX_MODELS = '/content/drive/My Drive/GoogleColab/AUX_MODELS/'\n",
        "\n",
        "\n",
        "#STORES ids\n",
        "#STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
        "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_evaluation.csv')['store_id']\n",
        "STORES_IDS = list(STORES_IDS.unique())\n",
        "\n",
        "\n",
        "#SPLITS for lags creation\n",
        "SHIFT_DAY  = 28\n",
        "N_LAGS     = 15\n",
        "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
        "ROLS_SPLIT = []\n",
        "for i in [1,7,14]:\n",
        "    for j in [7,14,30,60]:\n",
        "        ROLS_SPLIT.append([i,j])"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcAc6l1djRLp"
      },
      "source": [
        "if USE_AUX:\n",
        "    lgb_params['n_estimators'] = 2\n",
        "    \n",
        "# Some 'logs' that can compare\n",
        "#Train CA_1\n",
        "#[100]\tvalid_0's rmse: 2.02289\n",
        "#[200]\tvalid_0's rmse: 2.0017\n",
        "#[300]\tvalid_0's rmse: 1.99239\n",
        "#[400]\tvalid_0's rmse: 1.98471\n",
        "#[500]\tvalid_0's rmse: 1.97923\n",
        "#[600]\tvalid_0's rmse: 1.97284\n",
        "#[700]\tvalid_0's rmse: 1.96763\n",
        "#[800]\tvalid_0's rmse: 1.9624\n",
        "#[900]\tvalid_0's rmse: 1.95673\n",
        "#[1000]\tvalid_0's rmse: 1.95201\n",
        "#[1100]\tvalid_0's rmse: 1.9476\n",
        "#[1200]\tvalid_0's rmse: 1.9434\n",
        "#[1300]\tvalid_0's rmse: 1.9392\n",
        "#[1400]\tvalid_0's rmse: 1.93446\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvSNyJODNvGd"
      },
      "source": [
        "STORES_IDS=['CA_1'] # for testing purpose, only one store (CA_1) is used."
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-PrXxUYjRUS",
        "outputId": "aa10bade-47b5-4cce-b197-c3ccd0b257d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for store_id in STORES_IDS:\n",
        "    print('Train', store_id)\n",
        "    \n",
        "    # Get grid for current store\n",
        "    grid_df, features_columns = get_data_by_store(store_id)\n",
        "    \n",
        "    # Masks for \n",
        "    # Train (All data less than 1913)\n",
        "    # \"Validation\" (Last 28 days - not real validatio set)\n",
        "    # Test (All data greater than 1913 day, \n",
        "    #       with some gap for recursive features)\n",
        "    train_mask = grid_df['d']<=END_TRAIN\n",
        "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
        "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
        "    \n",
        "    # Apply masks and save lgb dataset as bin\n",
        "    # to reduce memory spikes during dtype convertations\n",
        "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
        "\n",
        "    # \"To avoid any conversions, we should always use np.float32\"\n",
        "    # or save to bin before start training\n",
        "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
        "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
        "                       label=grid_df[train_mask][TARGET])\n",
        "    train_data.save_binary('train_data.bin')\n",
        "    train_data = lgb.Dataset('train_data.bin')\n",
        "    \n",
        "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
        "                       label=grid_df[valid_mask][TARGET])\n",
        "    \n",
        "    # Saving part of the dataset for later predictions\n",
        "    # Removing features that we need to calculate recursively \n",
        "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
        "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
        "    grid_df = grid_df[keep_cols]\n",
        "    grid_df.to_pickle(path+'test_'+store_id+'.pkl')\n",
        "    del grid_df\n",
        "    \n",
        "    # Launch seeder again to make lgb training 100% deterministic\n",
        "    # with each \"code line\" np.random \"evolves\" \n",
        "    # so we need (may want) to \"reset\" it\n",
        "    seed_everything(SEED)\n",
        "    estimator = lgb.train(lgb_params,\n",
        "                          train_data,\n",
        "                          valid_sets = [valid_data],\n",
        "                          verbose_eval = 100,\n",
        "                          )\n",
        "    \n",
        "    # Save model - it's not real '.bin' but a pickle file\n",
        "    # estimator = lgb.Booster(model_file='model.txt')\n",
        "    # can only predict with the best iteration (or the saving iteration)\n",
        "    # pickle.dump gives us more flexibility\n",
        "    # like estimator.predict(TEST, num_iteration=100)\n",
        "    # num_iteration - number of iteration want to predict with, \n",
        "    # NULL or <= 0 means use best iteration\n",
        "    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
        "    pickle.dump(estimator, open(AUX_MODELS+model_name, 'wb'))\n",
        "\n",
        "    # Remove temporary files and objects \n",
        "    # to free some hdd space and ram memory\n",
        "    !rm train_data.bin\n",
        "    del train_data, valid_data, estimator\n",
        "    gc.collect()\n",
        "    \n",
        "    # \"Keep\" models features for predictions\n",
        "    MODEL_FEATURES = features_columns"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train CA_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-R-k506fXco"
      },
      "source": [
        "# Part 4.5 Final forecasting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5MPJ_i7jRIc",
        "outputId": "d9b36577-ee52-4d18-cd8f-09ca4bce2028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#USE_AUX=False\n",
        "#Predict Model\n",
        "all_preds = pd.DataFrame()\n",
        "\n",
        "base_test = get_base_test()\n",
        "\n",
        "main_time = time.time()\n",
        "\n",
        "for PREDICT_DAY in range(1,29):    \n",
        "    print('Predict | Day:', PREDICT_DAY)\n",
        "    start_time = time.time()\n",
        "\n",
        "    grid_df = base_test.copy()\n",
        "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "        \n",
        "    for store_id in STORES_IDS:\n",
        "\n",
        "        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
        "        if USE_AUX:\n",
        "            model_path = AUX_MODELS + model_path\n",
        "        \n",
        "        estimator = pickle.load(open(model_path, 'rb'))\n",
        "        #estimator=pi\n",
        "        \n",
        "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
        "        store_mask = base_test['store_id']==store_id\n",
        "        \n",
        "        mask = (day_mask)&(store_mask)\n",
        "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "\n",
        "    temp_df = base_test[day_mask][['id',TARGET]]\n",
        "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "    if 'id' in list(all_preds):\n",
        "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "    else:\n",
        "        all_preds = temp_df.copy()\n",
        "        \n",
        "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    del temp_df\n",
        "    \n",
        "all_preds = all_preds.reset_index(drop=True)\n",
        "all_preds"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict | Day: 1\n",
            "##########  0.25 min round |  0.25 min total |  594.59 day sales |\n",
            "Predict | Day: 2\n",
            "##########  0.25 min round |  0.49 min total |  595.16 day sales |\n",
            "Predict | Day: 3\n",
            "##########  0.25 min round |  0.74 min total |  594.98 day sales |\n",
            "Predict | Day: 4\n",
            "##########  0.25 min round |  0.98 min total |  594.78 day sales |\n",
            "Predict | Day: 5\n",
            "##########  0.25 min round |  1.23 min total |  596.91 day sales |\n",
            "Predict | Day: 6\n",
            "##########  0.24 min round |  1.47 min total |  603.99 day sales |\n",
            "Predict | Day: 7\n",
            "##########  0.25 min round |  1.72 min total |  603.96 day sales |\n",
            "Predict | Day: 8\n",
            "##########  0.25 min round |  1.96 min total |  594.47 day sales |\n",
            "Predict | Day: 9\n",
            "##########  0.25 min round |  2.21 min total |  594.46 day sales |\n",
            "Predict | Day: 10\n",
            "##########  0.24 min round |  2.45 min total |  594.35 day sales |\n",
            "Predict | Day: 11\n",
            "##########  0.25 min round |  2.70 min total |  594.18 day sales |\n",
            "Predict | Day: 12\n",
            "##########  0.25 min round |  2.95 min total |  596.01 day sales |\n",
            "Predict | Day: 13\n",
            "##########  0.25 min round |  3.20 min total |  603.42 day sales |\n",
            "Predict | Day: 14\n",
            "##########  0.26 min round |  3.46 min total |  603.02 day sales |\n",
            "Predict | Day: 15\n",
            "##########  0.29 min round |  3.75 min total |  592.93 day sales |\n",
            "Predict | Day: 16\n",
            "##########  0.33 min round |  4.08 min total |  593.04 day sales |\n",
            "Predict | Day: 17\n",
            "##########  0.48 min round |  4.56 min total |  592.55 day sales |\n",
            "Predict | Day: 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-48:\n",
            "Process ForkPoolWorker-47:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
            "    return list(map(*args))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-104-6f8da7eef733>\", line 50, in make_lag_roll\n",
            "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-fd5f66042223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mgrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mgrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_parallelize_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_lag_roll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROLS_SPLIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstore_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTORES_IDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-b5a5a731cbe0>\u001b[0m in \u001b[0;36mdf_parallelize_run\u001b[0;34m(func, t_split)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnum_cores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN_CORES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/generic.py\", line 494, in transform\n",
            "    func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/generic.py\", line 537, in _transform_general\n",
            "    res = func(group, *args, **kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
            "    return list(map(*args))\n",
            "  File \"<ipython-input-104-6f8da7eef733>\", line 50, in make_lag_roll\n",
            "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/generic.py\", line 494, in transform\n",
            "    func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/generic.py\", line 537, in _transform_general\n",
            "    res = func(group, *args, **kwargs)\n",
            "  File \"<ipython-input-104-6f8da7eef733>\", line 50, in <lambda>\n",
            "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/window/rolling.py\", line 2091, in mean\n",
            "    return super().mean(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/window/rolling.py\", line 1488, in mean\n",
            "    return self._apply(window_func, center=self.center, name=\"mean\", **kwargs)\n",
            "  File \"<ipython-input-104-6f8da7eef733>\", line 50, in <lambda>\n",
            "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\", line 4575, in shift\n",
            "    periods=periods, freq=freq, axis=axis, fill_value=fill_value\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/window/rolling.py\", line 599, in _apply\n",
            "    return self._wrap_results(results, block_list, obj, exclude)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/window/rolling.py\", line 400, in _wrap_results\n",
            "    result = self._wrap_result(result, block=block, obj=obj)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\", line 9154, in shift\n",
            "    return self._constructor(new_data).__finalize__(self, method=\"shift\")\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/window/rolling.py\", line 379, in _wrap_result\n",
            "    return Series(result, index, name=obj.name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\", line 213, in __init__\n",
            "    self.name = name\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\", line 327, in __init__\n",
            "    data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\", line 5151, in __setattr__\n",
            "    pass\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/construction.py\", line 428, in sanitize_array\n",
            "    subarr = _try_cast(data, dtype, copy, raise_cast_failure)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/construction.py\", line 539, in _try_cast\n",
            "    if maybe_castable(arr) and not copy and dtype is None:\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ukLF6pskX2O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J16aQvGNRvwC"
      },
      "source": [
        "***Since RAM got full, therefore it need to interrupt the execution while the model was predicting successfully upto days 17.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKNeAffykY0T",
        "outputId": "a856b378-7e18-44fb-cd42-93bb391305f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "all_preds"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>F9</th>\n",
              "      <th>F10</th>\n",
              "      <th>F11</th>\n",
              "      <th>F12</th>\n",
              "      <th>F13</th>\n",
              "      <th>F14</th>\n",
              "      <th>F15</th>\n",
              "      <th>F16</th>\n",
              "      <th>F17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.979966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.979966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.979966</td>\n",
              "      <td>0.979966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.979966</td>\n",
              "      <td>0.979966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.971863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.979703</td>\n",
              "      <td>0.969863</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.974684</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.990237</td>\n",
              "      <td>0.979966</td>\n",
              "      <td>0.976968</td>\n",
              "      <td>0.979966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38666235</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.011704</td>\n",
              "      <td>1.011704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38666236</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.010994</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.010994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38666237</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.010994</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.011704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38666238</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.010994</td>\n",
              "      <td>1.011704</td>\n",
              "      <td>1.010994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38666239</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.040883</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.024632</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.018203</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.042537</td>\n",
              "      <td>1.010994</td>\n",
              "      <td>1.011704</td>\n",
              "      <td>1.011704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>38666240 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     id        F1  ...       F16       F17\n",
              "0         HOBBIES_1_010_CA_1_validation  0.971863  ...  0.976968  0.979966\n",
              "1         HOBBIES_1_010_CA_1_validation  0.971863  ...  0.976968  0.979966\n",
              "2         HOBBIES_1_010_CA_1_validation  0.971863  ...  0.979966  0.979966\n",
              "3         HOBBIES_1_010_CA_1_validation  0.971863  ...  0.979966  0.979966\n",
              "4         HOBBIES_1_010_CA_1_validation  0.971863  ...  0.976968  0.979966\n",
              "...                                 ...       ...  ...       ...       ...\n",
              "38666235    FOODS_3_820_CA_1_validation  1.040883  ...  1.011704  1.011704\n",
              "38666236    FOODS_3_820_CA_1_validation  1.040883  ...  1.018203  1.010994\n",
              "38666237    FOODS_3_820_CA_1_validation  1.040883  ...  1.018203  1.011704\n",
              "38666238    FOODS_3_820_CA_1_validation  1.040883  ...  1.011704  1.010994\n",
              "38666239    FOODS_3_820_CA_1_validation  1.040883  ...  1.011704  1.011704\n",
              "\n",
              "[38666240 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxFiFd4BRiXi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}