{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5 Final_notebook_Oct12_13.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "100oqc5jqKv6jrNIl59YPc3dIhi0m6U0U",
      "authorship_tag": "ABX9TyMPDsE3k9ibMAJ2V1+kaEC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamalchapagain/Kaggle_competition_solutions/blob/master/M5_Final_notebook_Oct12_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ymgEtywY3kN"
      },
      "source": [
        "# Summary of this notebook\n",
        "\n",
        "**This notebook develope the M5-accuracy model, it is divided into four main parts **\n",
        "- Part 1. Simple Feature Engineering,\n",
        "    - Sampling of dataset\n",
        "    - Downcast of memory\n",
        "    - Melting\n",
        "    - Statistical features of sales: min/max/mean/std\n",
        "    - Normalization (min/max scaling)\n",
        "    - rolling aggregation\n",
        "    - momentum\n",
        "- Part 2. Lag Analysis,\n",
        "- Part 3: Mean encoding (Custom encoding)\n",
        "    - FE creation approaches\n",
        "    - Baseline model for FE validation\n",
        "    - Implementation of PCA for Dimension reduction\n",
        "    - FE validation by Permutation importance\n",
        "\n",
        "- Part 4: Final forecasting\n",
        "    - parallelization for FE\n",
        "    - get the final_processed datasets\n",
        "    - model selection (implementation)\n",
        "    - Train the model (beware of data leakage)\n",
        "    - Final forecasting\n",
        "\n",
        "- ***The improvement on the forecasting performance should come from***\n",
        "    - Loss function\n",
        "    - Data representation\n",
        "    - Stable CV\n",
        "    - Good features reduction strategy\n",
        "    - Predictions stabilization with NN\n",
        "    - Trend prediction\n",
        "    - Real zero sales detection/classification\n",
        "\n",
        "- *** The features created in these notebook is from***\n",
        "    - **Mean encodings and PCA options** *https://www.kaggle.com/kyakovlev/m5-custom-features*\n",
        "    - **Lags and rolling lags** https://www.kaggle.com/kyakovlev/m5-lags-features*\n",
        "    - **Base Grid and base features (calendar/price/etc)** *https://www.kaggle.com/kyakovlev/m5-simple-fe*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K6s4Zf1aAnW",
        "outputId": "05c7a0ec-700a-4bd1-c9d5-ea92c3f07580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#for plot\n",
        "!pip install plotly"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSlF3azXZCD-"
      },
      "source": [
        "#Loading of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJocMax0Y43i"
      },
      "source": [
        "# General imports of library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDP05oYnZM6N",
        "outputId": "402fdff9-8768-4317-b5a3-95fe0656cd0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "########################### Load Data#####################\n",
        "print('Loading the sampled Data')\n",
        "path='/content/drive/My Drive/GoogleColab/'\n",
        "train_df = pd.read_csv(path+'sales_train_evaluation.csv')\n",
        "prices_df = pd.read_csv(path+'sell_prices.csv')\n",
        "calendar_df =pd.read_csv(path+'calendar.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the sampled Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJfq4FKtZS_T",
        "outputId": "81c2a879-75eb-4bfd-bb54-f3c7263dbb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "train_df.head(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d_1</th>\n",
              "      <th>d_2</th>\n",
              "      <th>d_3</th>\n",
              "      <th>d_4</th>\n",
              "      <th>d_5</th>\n",
              "      <th>d_6</th>\n",
              "      <th>d_7</th>\n",
              "      <th>d_8</th>\n",
              "      <th>d_9</th>\n",
              "      <th>d_10</th>\n",
              "      <th>d_11</th>\n",
              "      <th>d_12</th>\n",
              "      <th>d_13</th>\n",
              "      <th>d_14</th>\n",
              "      <th>d_15</th>\n",
              "      <th>d_16</th>\n",
              "      <th>d_17</th>\n",
              "      <th>d_18</th>\n",
              "      <th>d_19</th>\n",
              "      <th>d_20</th>\n",
              "      <th>d_21</th>\n",
              "      <th>d_22</th>\n",
              "      <th>d_23</th>\n",
              "      <th>d_24</th>\n",
              "      <th>d_25</th>\n",
              "      <th>d_26</th>\n",
              "      <th>d_27</th>\n",
              "      <th>d_28</th>\n",
              "      <th>d_29</th>\n",
              "      <th>d_30</th>\n",
              "      <th>d_31</th>\n",
              "      <th>d_32</th>\n",
              "      <th>d_33</th>\n",
              "      <th>d_34</th>\n",
              "      <th>...</th>\n",
              "      <th>d_1902</th>\n",
              "      <th>d_1903</th>\n",
              "      <th>d_1904</th>\n",
              "      <th>d_1905</th>\n",
              "      <th>d_1906</th>\n",
              "      <th>d_1907</th>\n",
              "      <th>d_1908</th>\n",
              "      <th>d_1909</th>\n",
              "      <th>d_1910</th>\n",
              "      <th>d_1911</th>\n",
              "      <th>d_1912</th>\n",
              "      <th>d_1913</th>\n",
              "      <th>d_1914</th>\n",
              "      <th>d_1915</th>\n",
              "      <th>d_1916</th>\n",
              "      <th>d_1917</th>\n",
              "      <th>d_1918</th>\n",
              "      <th>d_1919</th>\n",
              "      <th>d_1920</th>\n",
              "      <th>d_1921</th>\n",
              "      <th>d_1922</th>\n",
              "      <th>d_1923</th>\n",
              "      <th>d_1924</th>\n",
              "      <th>d_1925</th>\n",
              "      <th>d_1926</th>\n",
              "      <th>d_1927</th>\n",
              "      <th>d_1928</th>\n",
              "      <th>d_1929</th>\n",
              "      <th>d_1930</th>\n",
              "      <th>d_1931</th>\n",
              "      <th>d_1932</th>\n",
              "      <th>d_1933</th>\n",
              "      <th>d_1934</th>\n",
              "      <th>d_1935</th>\n",
              "      <th>d_1936</th>\n",
              "      <th>d_1937</th>\n",
              "      <th>d_1938</th>\n",
              "      <th>d_1939</th>\n",
              "      <th>d_1940</th>\n",
              "      <th>d_1941</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
              "      <td>HOBBIES_1_002</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id        item_id  ... d_1940 d_1941\n",
              "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  ...      0      1\n",
              "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  ...      0      0\n",
              "\n",
              "[2 rows x 1947 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOcN3XpOZpcK"
      },
      "source": [
        "**This this dataset upto 1913 is used for validation purpose, therefor, for simplicity to undersand, we are going to change the column *id* contains *_evaluation into* _validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwhoAWaRZjgC"
      },
      "source": [
        "import re\n",
        "train_df.id.replace({'evaluation':'validation'}, regex=True, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXrivgDZZ6c4",
        "outputId": "7f652570-18eb-45f2-9628-dc0a4b9c9a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d_1</th>\n",
              "      <th>d_2</th>\n",
              "      <th>d_3</th>\n",
              "      <th>d_4</th>\n",
              "      <th>d_5</th>\n",
              "      <th>d_6</th>\n",
              "      <th>d_7</th>\n",
              "      <th>d_8</th>\n",
              "      <th>d_9</th>\n",
              "      <th>d_10</th>\n",
              "      <th>d_11</th>\n",
              "      <th>d_12</th>\n",
              "      <th>d_13</th>\n",
              "      <th>d_14</th>\n",
              "      <th>d_15</th>\n",
              "      <th>d_16</th>\n",
              "      <th>d_17</th>\n",
              "      <th>d_18</th>\n",
              "      <th>d_19</th>\n",
              "      <th>d_20</th>\n",
              "      <th>d_21</th>\n",
              "      <th>d_22</th>\n",
              "      <th>d_23</th>\n",
              "      <th>d_24</th>\n",
              "      <th>d_25</th>\n",
              "      <th>d_26</th>\n",
              "      <th>d_27</th>\n",
              "      <th>d_28</th>\n",
              "      <th>d_29</th>\n",
              "      <th>d_30</th>\n",
              "      <th>d_31</th>\n",
              "      <th>d_32</th>\n",
              "      <th>d_33</th>\n",
              "      <th>d_34</th>\n",
              "      <th>...</th>\n",
              "      <th>d_1902</th>\n",
              "      <th>d_1903</th>\n",
              "      <th>d_1904</th>\n",
              "      <th>d_1905</th>\n",
              "      <th>d_1906</th>\n",
              "      <th>d_1907</th>\n",
              "      <th>d_1908</th>\n",
              "      <th>d_1909</th>\n",
              "      <th>d_1910</th>\n",
              "      <th>d_1911</th>\n",
              "      <th>d_1912</th>\n",
              "      <th>d_1913</th>\n",
              "      <th>d_1914</th>\n",
              "      <th>d_1915</th>\n",
              "      <th>d_1916</th>\n",
              "      <th>d_1917</th>\n",
              "      <th>d_1918</th>\n",
              "      <th>d_1919</th>\n",
              "      <th>d_1920</th>\n",
              "      <th>d_1921</th>\n",
              "      <th>d_1922</th>\n",
              "      <th>d_1923</th>\n",
              "      <th>d_1924</th>\n",
              "      <th>d_1925</th>\n",
              "      <th>d_1926</th>\n",
              "      <th>d_1927</th>\n",
              "      <th>d_1928</th>\n",
              "      <th>d_1929</th>\n",
              "      <th>d_1930</th>\n",
              "      <th>d_1931</th>\n",
              "      <th>d_1932</th>\n",
              "      <th>d_1933</th>\n",
              "      <th>d_1934</th>\n",
              "      <th>d_1935</th>\n",
              "      <th>d_1936</th>\n",
              "      <th>d_1937</th>\n",
              "      <th>d_1938</th>\n",
              "      <th>d_1939</th>\n",
              "      <th>d_1940</th>\n",
              "      <th>d_1941</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_001</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_002</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_003</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_004</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_005</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id        item_id  ... d_1940 d_1941\n",
              "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  ...      0      1\n",
              "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  ...      0      0\n",
              "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  ...      0      1\n",
              "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  ...      2      6\n",
              "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  ...      1      0\n",
              "\n",
              "[5 rows x 1947 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT1Ysd_NapV3"
      },
      "source": [
        "#Technique 1. Sampling of Dataset:\n",
        "  - **Category:** Sampling technique is categorized to the pre-processing stage where we can have sampled dataset from the population dataset.\n",
        "  - **Description:** Large volumes of data need huge computation time, and possibility of memory exploded. Therefore, the analysis of a small dataset extracted from the original set is preferred. From this smal set, called sampled dataset, approximate result can be obtained.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofPXT5vZamW-",
        "outputId": "ffcc01a2-7fd3-4f61-d7ea-1ef150489454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Reduction of dataset (just take 10% of dataset)\n",
        "from sklearn.model_selection import train_test_split #library\n",
        "\n",
        "def sampling_dataset(dataset):\n",
        "# dataset: pandas dataframe for the population\n",
        "# data_keep: sampled dataframe based on train_test_split (we will keep this for further processing)\n",
        "# data_discard: sampled datafram that we remove (we will remove this dataset)\n",
        "# train_size=0.1  : samples 10% of dataset from the popolation and store at variable data_keep\n",
        "# random_state=42 : to reproduce same result\n",
        "\n",
        "  data_keep, data_discard=train_test_split(dataset,train_size=0.1, random_state=42)\n",
        "  print('Total size and sampled size')\n",
        "  print(dataset.shape,data_keep.shape)\n",
        "  data_keep.sort_index(inplace=True)\n",
        "  data_keep.reset_index(drop=True, inplace=True)\n",
        "  return data_keep\n",
        "\n",
        "#eg-\n",
        "train_df=sampling_dataset(train_df)\n",
        "prices_df=sampling_dataset(prices_df)\n",
        "train_df_for_lag_feature=train_df.copy() #Lets save train_df content for part 2.1 (lag features analysis)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total size and sampled size\n",
            "(30490, 1947) (3049, 1947)\n",
            "Total size and sampled size\n",
            "(6841121, 4) (684112, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn-cGfBiJlev"
      },
      "source": [
        "########################### Vars ##############\n",
        "TARGET = 'sales'         # Our main target\n",
        "END_TRAIN = 1913         # Last day in train set, However new dataset contains upto d_1941\n",
        "MAIN_INDEX = ['id','d']  # We can identify item by these columns\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEhmQc_DdMoj"
      },
      "source": [
        "***Due to computational limitations, we are just taking 10% of dataset. Few bloggers talking about 'Stratified sampling', however we implement train_test_split strategy with random state.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBAWJ8Hvex8N"
      },
      "source": [
        "#Some user-defined functions,\n",
        "\n",
        "## Simple \"Memory profilers\" to see memory usage\n",
        "def get_memory_usage():\n",
        "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
        "        \n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "## Merging by concat to not lose dtypes\n",
        "def merge_by_concat(df1, df2, merge_on):\n",
        "    merged_gf = df1[merge_on]\n",
        "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
        "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
        "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
        "    return df1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZFfbLEpjd40"
      },
      "source": [
        "# Part 1.1 Downcasting the memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7U80nI7jciq"
      },
      "source": [
        "## Downcasting the memory \n",
        "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
        "# :verbose                                        # type: bool\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                       df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3wmUgRKefbH"
      },
      "source": [
        "# Part 1.3  Melting the dataset\n",
        "- In this section the we transform the horizontal dataset into the vertical view.\n",
        "\n",
        "- Out index will be *'id'*, *'item_id'*, *'dept_id'*, *'cat_id'*, *'store_id'*, *'state_id'* and labels are *'d_'* coulmns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aApfdA2QdKca",
        "outputId": "19f5daee-8709-49fa-ee67-8c098f14215c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Making Grids (for prices/sales) ###################################\n",
        "print('Melting the dataset to different grids')\n",
        "\n",
        "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
        "grid_df = pd.melt(train_df, \n",
        "                  id_vars = index_columns, \n",
        "                  var_name = 'd', \n",
        "                  value_name = TARGET)\n",
        "\n",
        "\n",
        "#After melting, lets observe the train rows and columns\n",
        "print('Train rows:', len(train_df), len(grid_df))\n",
        "\n",
        "\n",
        "# To be able to make predictions we need to add \"test set\" to our grid\n",
        "add_grid = pd.DataFrame()\n",
        "for i in range(1,29):\n",
        "    temp_df = train_df[index_columns]\n",
        "    temp_df = temp_df.drop_duplicates()\n",
        "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
        "    temp_df[TARGET] = np.nan\n",
        "    add_grid = pd.concat([add_grid,temp_df])\n",
        "\n",
        "grid_df = pd.concat([grid_df,add_grid])\n",
        "grid_df = grid_df.reset_index(drop=True)\n",
        "\n",
        "# Remove some temoprary DFs\n",
        "del temp_df, add_grid\n",
        "\n",
        "# We will not need original train_df anymore and can remove it\n",
        "del train_df\n",
        "\n",
        "# Let's check our memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Size of grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Melting the dataset to different grids\n",
            "Train rows: 3049 5918109\n",
            "     Size of grid_df: 366.4MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL698Am6fm8c"
      },
      "source": [
        "*We can free some memory by converting \"strings\" to categorical which will not affect merging and we will not lose any valuable data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2tW0LdCescd",
        "outputId": "fbf79ab5-a451-4ed6-9a6a-2e2df063c7f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for col in index_columns:\n",
        "    grid_df[col] = grid_df[col].astype('category')\n",
        "\n",
        "# Let's check again memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Reduced grid_df: 137.6MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joWPXuC7gGjL"
      },
      "source": [
        "- ***The important thing that need to understand is that the leadings zero values in each*** *train_df*  ***item row are not real 0 sales but mean absence for the item in the store. Such zeros can remove to save some memory***\n",
        "\n",
        "\n",
        "#Some Feature Engg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAb6rflVgBon",
        "outputId": "bedeb42c-4c79-4354-fbbc-3ec5c2416682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Product Release date ###############################\n",
        "print('Release week')\n",
        "\n",
        "# Prices are set by week so it we will have not very accurate release week \n",
        "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
        "release_df.columns = ['store_id','item_id','release']\n",
        "\n",
        "# Now we can merge release_df\n",
        "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
        "del release_df\n",
        "\n",
        "# We want to remove some \"zeros\" rows from grid_df to do it we need wm_yr_wk column\n",
        "# let's merge partly calendar_df to have it\n",
        "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
        "                      \n",
        "# Now we can cutoff some rows and safe memory \n",
        "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
        "grid_df = grid_df.reset_index(drop=True)\n",
        "\n",
        "# Let's check our memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "\n",
        "# Should we keep release week as one of the features? for now, lets keep release feature by reducing the memory size int16.\n",
        "\n",
        "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
        "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
        "\n",
        "# Let's check again memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Release week\n",
            "    Original grid_df: 174.9MiB\n",
            "     Reduced grid_df: 148.7MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePhj0ZJegr-l",
        "outputId": "732c986d-6f78-4d80-e487-88d26fe6bb3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Save part 1 ################\n",
        "print('Save Part 1')\n",
        "print('Part 1 is ready')\n",
        "\n",
        "# We have our BASE grid ready and can save it as pickle file for future use (model training)\n",
        "grid_df.to_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_1.pkl')\n",
        "\n",
        "print('Size:', grid_df.shape)\n",
        "grid_part_1=grid_df.copy() #save in the name of grid_part_1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Part 1\n",
            "Part 1 is ready\n",
            "Size: (4578794, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr7LKTPrg4PQ",
        "outputId": "0f376670-8c7c-4f29-8eeb-134b603c2790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 10 columns):\n",
            " #   Column    Dtype   \n",
            "---  ------    -----   \n",
            " 0   id        category\n",
            " 1   item_id   category\n",
            " 2   dept_id   category\n",
            " 3   cat_id    category\n",
            " 4   store_id  category\n",
            " 5   state_id  category\n",
            " 6   d         object  \n",
            " 7   sales     float64 \n",
            " 8   release   int16   \n",
            " 9   wm_yr_wk  int64   \n",
            "dtypes: category(6), float64(1), int16(1), int64(1), object(1)\n",
            "memory usage: 148.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHsvdtnphCsG"
      },
      "source": [
        "#Part 2. Feature Engineering\n",
        "- **Some basic FE for prices are:** \n",
        "- statistical feature for prices are: min_price, max_price, average price, deviation in price\n",
        "- Normalization (min/max scaling)\n",
        "- \"rolling\" aggregations but would like months and years as \"window\"\n",
        "- Momentum of the prices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8OHeWeUg8GS",
        "outputId": "dd6ac09e-b2f9-4c0c-8abe-edd55cf92cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "########################### Prices ############################################\n",
        "print('Feature Engineering of Prices')\n",
        "\n",
        "# We can do some basic aggregations\n",
        "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
        "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
        "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
        "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
        "\n",
        "# and do price normalization (min/max scaling)\n",
        "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
        "\n",
        "# Some items are can be inflation dependent and some items are very \"stable\"\n",
        "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
        "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
        "\n",
        "# we would like some \"rolling\" aggregations but would like months and years as \"window\"\n",
        "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
        "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
        "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
        "del calendar_prices\n",
        "\n",
        "# Now we can add price \"momentum\" (some sort of) Shifted by week, by month mean, and by year mean\n",
        "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
        "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
        "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
        "\n",
        "del prices_df['month'], prices_df['year']\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Engineering of Prices\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9gdopKuyV1H",
        "outputId": "902216ab-15a5-4d96-c1f2-17a31a779844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Merge prices and save part 2 #################################################\n",
        "print('Merge prices and save part 2')\n",
        "\n",
        "# Merge Prices\n",
        "original_columns = list(grid_df)\n",
        "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
        "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
        "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
        "grid_df = reduce_mem_usage(grid_df)\n",
        "\n",
        "# Safe part 2\n",
        "grid_df.to_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_2.pkl')\n",
        "print('Size:', grid_df.shape)\n",
        "grid_part_2=grid_df.copy()\n",
        "\n",
        "# We don't need prices_df anymore\n",
        "del prices_df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Merge prices and save part 2\n",
            "Mem. usage decreased to 174.77 Mb (62.3% reduction)\n",
            "Size: (4578794, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO5syngBhr7i",
        "outputId": "82e79038-f32d-44b9-982f-bb4ad85e18d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "grid_part_2.info()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 13 columns):\n",
            " #   Column            Dtype   \n",
            "---  ------            -----   \n",
            " 0   id                category\n",
            " 1   d                 object  \n",
            " 2   sell_price        float16 \n",
            " 3   price_max         float16 \n",
            " 4   price_min         float16 \n",
            " 5   price_std         float16 \n",
            " 6   price_mean        float16 \n",
            " 7   price_norm        float16 \n",
            " 8   price_nunique     float16 \n",
            " 9   item_nunique      float16 \n",
            " 10  price_momentum    float16 \n",
            " 11  price_momentum_m  float16 \n",
            " 12  price_momentum_y  float16 \n",
            "dtypes: category(1), float16(11), object(1)\n",
            "memory usage: 174.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV7xNUrnjAQe",
        "outputId": "32cdcfc7-f53d-4f93-8415-54696d6e6aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "calendar_df.columns"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'd',\n",
              "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
              "       'snap_CA', 'snap_TX', 'snap_WI'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CZaCRRSpJe8"
      },
      "source": [
        "\n",
        "########################### Lets Merge the calendar ###########################\n",
        "\n",
        "grid_df = grid_df[MAIN_INDEX]\n",
        "\n",
        "# Merge calendar partly \n",
        "icols = ['date',\n",
        "         'd',\n",
        "         'event_name_1',\n",
        "         'event_type_1',\n",
        "         'event_name_2',\n",
        "         'event_type_2',\n",
        "         'snap_CA',\n",
        "         'snap_TX',\n",
        "         'snap_WI']\n",
        "\n",
        "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
        "\n",
        "# Again, lets minify data\n",
        "# 'snap_' columns we can convert to bool or int8\n",
        "icols = ['event_name_1',\n",
        "         'event_type_1',\n",
        "         'event_name_2',\n",
        "         'event_type_2',\n",
        "         'snap_CA',\n",
        "         'snap_TX',\n",
        "         'snap_WI']\n",
        "for col in icols:\n",
        "    grid_df[col] = grid_df[col].astype('category')\n",
        "\n",
        "# Convert to DateTime\n",
        "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
        "\n",
        "# Make some features from date\n",
        "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
        "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
        "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
        "grid_df['tm_y'] = grid_df['date'].dt.year\n",
        "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
        "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
        "\n",
        "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
        "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
        "\n",
        "# Remove date\n",
        "del grid_df['date']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QecjHC8pWri",
        "outputId": "da80acb1-19d4-4363-b13e-af9b52c22bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "########################### Save part 3 (Dates) ###########################\n",
        "print('Save part 3')\n",
        "\n",
        "grid_df.to_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_3.pkl')\n",
        "print('Size:', grid_df.shape)\n",
        "\n",
        "grid_part_3=grid_df.copy()\n",
        "\n",
        "del calendar_df\n",
        "del grid_df"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save part 3\n",
            "Size: (4578794, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5_PAIWkpOFb",
        "outputId": "8992df9f-bc68-4cc8-fc78-342a0122c4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "grid_part_3.info()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Dtype   \n",
            "---  ------        -----   \n",
            " 0   id            category\n",
            " 1   d             object  \n",
            " 2   event_name_1  category\n",
            " 3   event_type_1  category\n",
            " 4   event_name_2  category\n",
            " 5   event_type_2  category\n",
            " 6   snap_CA       category\n",
            " 7   snap_TX       category\n",
            " 8   snap_WI       category\n",
            " 9   tm_d          int8    \n",
            " 10  tm_w          int8    \n",
            " 11  tm_m          int8    \n",
            " 12  tm_y          int8    \n",
            " 13  tm_wm         int8    \n",
            " 14  tm_dw         int8    \n",
            " 15  tm_w_end      int8    \n",
            "dtypes: category(8), int8(7), object(1)\n",
            "memory usage: 139.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXeOmG1BpS-k"
      },
      "source": [
        "########################### Some additional cleaning (converting 'd' into int)\n",
        "#################################################################################\n",
        "\n",
        "## Part 1\n",
        "# Convert 'd' to int\n",
        "grid_df = grid_part_1.copy()\n",
        "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
        "\n",
        "path='/content/drive/My Drive/GoogleColab/pickle_files/'\n",
        "grid_df.to_pickle(path+'grid_part_1.pkl')\n",
        "\n",
        "# Remove 'wm_yr_wk'\n",
        "# as test values are not in train set\n",
        "del grid_df['wm_yr_wk']\n",
        "grid_part_1=grid_df.copy()\n",
        "\n",
        "del grid_df"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbPXgb4ZpdeT",
        "outputId": "34c8d2eb-336d-43b6-af22-69cb0e808071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# lets concat all three grids for further processing\n",
        "\n",
        "grid_df=pd.concat([grid_part_1, grid_part_2.iloc[:,2:], grid_part_3.iloc[:,2:]], axis=1)\n",
        "\n",
        "# Let's check again memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "print('Size:', grid_df.shape)\n",
        "\n",
        "#           Full Grid:   157.4 MiB\n",
        "\n",
        "#Still memory usage is high, we can train by state_id or shop_id !!\n",
        "\n",
        "state_id = 'CA'\n",
        "grid_df = grid_df[grid_df['state_id']==state_id]\n",
        "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "# Full Grid:  75.3MiB\n",
        "\n",
        "store_id = 'CA_1'\n",
        "grid_df = grid_df[grid_df['store_id']==store_id]\n",
        "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
        "#           Now, Full Grid: 19.2 MiB"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Full Grid: 244.7MiB\n",
            "Size: (4578794, 34)\n",
            "           Full Grid: 109.4MiB\n",
            "           Full Grid:  27.8MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuE9IUCRpgrR",
        "outputId": "32ffcaa7-682d-47dd-8e5c-a31c295601b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "########################### Final list of features\n",
        "#################################################################################\n",
        "grid_df.info()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 451999 entries, 0 to 4576039\n",
            "Data columns (total 34 columns):\n",
            " #   Column            Non-Null Count   Dtype   \n",
            "---  ------            --------------   -----   \n",
            " 0   id                451999 non-null  category\n",
            " 1   item_id           451999 non-null  category\n",
            " 2   dept_id           451999 non-null  category\n",
            " 3   cat_id            451999 non-null  category\n",
            " 4   store_id          451999 non-null  category\n",
            " 5   state_id          451999 non-null  category\n",
            " 6   d                 451999 non-null  int16   \n",
            " 7   sales             443739 non-null  float64 \n",
            " 8   release           451999 non-null  int16   \n",
            " 9   sell_price        46182 non-null   float16 \n",
            " 10  price_max         46182 non-null   float16 \n",
            " 11  price_min         46182 non-null   float16 \n",
            " 12  price_std         46182 non-null   float16 \n",
            " 13  price_mean        46182 non-null   float16 \n",
            " 14  price_norm        46182 non-null   float16 \n",
            " 15  price_nunique     46182 non-null   float16 \n",
            " 16  item_nunique      46182 non-null   float16 \n",
            " 17  price_momentum    44117 non-null   float16 \n",
            " 18  price_momentum_m  46182 non-null   float16 \n",
            " 19  price_momentum_y  46182 non-null   float16 \n",
            " 20  event_name_1      37127 non-null   category\n",
            " 21  event_type_1      37127 non-null   category\n",
            " 22  event_name_2      863 non-null     category\n",
            " 23  event_type_2      863 non-null     category\n",
            " 24  snap_CA           451999 non-null  category\n",
            " 25  snap_TX           451999 non-null  category\n",
            " 26  snap_WI           451999 non-null  category\n",
            " 27  tm_d              451999 non-null  int8    \n",
            " 28  tm_w              451999 non-null  int8    \n",
            " 29  tm_m              451999 non-null  int8    \n",
            " 30  tm_y              451999 non-null  int8    \n",
            " 31  tm_wm             451999 non-null  int8    \n",
            " 32  tm_dw             451999 non-null  int8    \n",
            " 33  tm_w_end          451999 non-null  int8    \n",
            "dtypes: category(13), float16(11), float64(1), int16(2), int8(7)\n",
            "memory usage: 27.8 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEPxsS5uplE1"
      },
      "source": [
        "#Lag Feature\n",
        "#Lets load the sales dataset\n",
        "train_df=train_df_for_lag_feature.copy() #This file was already saved while sampling"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB1RadMKpzE3",
        "outputId": "4832e072-d172-4159-8414-948dc00a57b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To make all calculations faster, we will limit dataset by 'CA' state\n",
        "train_df = train_df[train_df['state_id']=='CA']\n",
        "print('Shape is: %s' %str(train_df.shape))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape is: (1202, 1947)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHFrw4l_p5EP"
      },
      "source": [
        "\n",
        "Here the number of features are d_1 to d_1941 (but we will consider upto d_1912) i.e a lot of feature, which is good, but we have just 1202 training rows (however we are taking only for CA)\n",
        "\n",
        "In other hand we can think of d_ columns as additional labels and can significantly scale up our training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUf-Vd-ep1mN",
        "outputId": "be4347c2-86b1-4698-b69a-9f105aff6c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "## Horizontal representation to vertical representation (tranformation)\n",
        "train_df.iloc[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d_1</th>\n",
              "      <th>d_2</th>\n",
              "      <th>d_3</th>\n",
              "      <th>d_4</th>\n",
              "      <th>d_5</th>\n",
              "      <th>d_6</th>\n",
              "      <th>d_7</th>\n",
              "      <th>d_8</th>\n",
              "      <th>d_9</th>\n",
              "      <th>d_10</th>\n",
              "      <th>d_11</th>\n",
              "      <th>d_12</th>\n",
              "      <th>d_13</th>\n",
              "      <th>d_14</th>\n",
              "      <th>d_15</th>\n",
              "      <th>d_16</th>\n",
              "      <th>d_17</th>\n",
              "      <th>d_18</th>\n",
              "      <th>d_19</th>\n",
              "      <th>d_20</th>\n",
              "      <th>d_21</th>\n",
              "      <th>d_22</th>\n",
              "      <th>d_23</th>\n",
              "      <th>d_24</th>\n",
              "      <th>d_25</th>\n",
              "      <th>d_26</th>\n",
              "      <th>d_27</th>\n",
              "      <th>d_28</th>\n",
              "      <th>d_29</th>\n",
              "      <th>d_30</th>\n",
              "      <th>d_31</th>\n",
              "      <th>d_32</th>\n",
              "      <th>d_33</th>\n",
              "      <th>d_34</th>\n",
              "      <th>...</th>\n",
              "      <th>d_1902</th>\n",
              "      <th>d_1903</th>\n",
              "      <th>d_1904</th>\n",
              "      <th>d_1905</th>\n",
              "      <th>d_1906</th>\n",
              "      <th>d_1907</th>\n",
              "      <th>d_1908</th>\n",
              "      <th>d_1909</th>\n",
              "      <th>d_1910</th>\n",
              "      <th>d_1911</th>\n",
              "      <th>d_1912</th>\n",
              "      <th>d_1913</th>\n",
              "      <th>d_1914</th>\n",
              "      <th>d_1915</th>\n",
              "      <th>d_1916</th>\n",
              "      <th>d_1917</th>\n",
              "      <th>d_1918</th>\n",
              "      <th>d_1919</th>\n",
              "      <th>d_1920</th>\n",
              "      <th>d_1921</th>\n",
              "      <th>d_1922</th>\n",
              "      <th>d_1923</th>\n",
              "      <th>d_1924</th>\n",
              "      <th>d_1925</th>\n",
              "      <th>d_1926</th>\n",
              "      <th>d_1927</th>\n",
              "      <th>d_1928</th>\n",
              "      <th>d_1929</th>\n",
              "      <th>d_1930</th>\n",
              "      <th>d_1931</th>\n",
              "      <th>d_1932</th>\n",
              "      <th>d_1933</th>\n",
              "      <th>d_1934</th>\n",
              "      <th>d_1935</th>\n",
              "      <th>d_1936</th>\n",
              "      <th>d_1937</th>\n",
              "      <th>d_1938</th>\n",
              "      <th>d_1939</th>\n",
              "      <th>d_1940</th>\n",
              "      <th>d_1941</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_012</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_014_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_014</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_056_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_056</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_062_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_062</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HOBBIES_1_080_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_080</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>HOBBIES_1_092_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_092</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>HOBBIES_1_110_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_110</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>HOBBIES_1_122_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_122</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>HOBBIES_1_130_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_130</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 1947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id        item_id  ... d_1940 d_1941\n",
              "0  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...      0      1\n",
              "1  HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  ...      1      0\n",
              "2  HOBBIES_1_014_CA_1_validation  HOBBIES_1_014  ...      1      3\n",
              "3  HOBBIES_1_056_CA_1_validation  HOBBIES_1_056  ...      0      0\n",
              "4  HOBBIES_1_062_CA_1_validation  HOBBIES_1_062  ...      1      1\n",
              "5  HOBBIES_1_080_CA_1_validation  HOBBIES_1_080  ...      0      0\n",
              "6  HOBBIES_1_092_CA_1_validation  HOBBIES_1_092  ...      0      0\n",
              "7  HOBBIES_1_110_CA_1_validation  HOBBIES_1_110  ...      0      0\n",
              "8  HOBBIES_1_122_CA_1_validation  HOBBIES_1_122  ...      0      4\n",
              "9  HOBBIES_1_130_CA_1_validation  HOBBIES_1_130  ...      1      3\n",
              "\n",
              "[10 rows x 1947 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxmWr9sYM5Qc"
      },
      "source": [
        "Lets go for Vertical representation:\n",
        "\n",
        "In other hand we can think of d_ columns as additional labels and can significantly\n",
        "scale up our training set to 2333082 rows with 8 columns\n",
        "\n",
        "Good thing that our model will have greater input for training\n",
        "\n",
        "Bad thing that we are losing lags that we had in horizontal representation and\n",
        "\n",
        "also new data set consumes much more memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcLwgZUNI5Ht",
        "outputId": "66e917f6-4cd1-4af3-c5a8-7f2bc7908be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
        "train_df = pd.melt(train_df, \n",
        "                  id_vars = index_columns, \n",
        "                  var_name = 'd', \n",
        "                  value_name = TARGET)\n",
        "\n",
        "train_df[train_df['id']=='HOBBIES_1_010_CA_1_validation'].iloc[:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3606</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4808</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6010</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7212</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9616</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10818</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_010</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>d_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id        item_id  ...     d sales\n",
              "0      HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_1     0\n",
              "1202   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_2     0\n",
              "2404   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_3     1\n",
              "3606   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_4     0\n",
              "4808   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_5     0\n",
              "6010   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_6     0\n",
              "7212   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_7     0\n",
              "8414   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_8     0\n",
              "9616   HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...   d_9     0\n",
              "10818  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  ...  d_10     0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptfndsI9M8Vd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OrNQGeSM_kG"
      },
      "source": [
        "Lags creation\n",
        "\n",
        "For the lag creation our dataset is allready sorted by d values\n",
        "now, we can simply shift() values also we have to keep in mind that we need to aggregate values on 'id' level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv9G-yLmNAdG",
        "outputId": "c9bf7de3-5404-4160-b984-0c156d9e2ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "########################### Lags creation ##########################################################\n",
        "\n",
        "# group and shift in loop\n",
        "temp_df = train_df[['id','d',TARGET]]\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(1,8):\n",
        "    print('Shifting:', i)\n",
        "    temp_df['lag_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(i))\n",
        "    \n",
        "print('%0.2f min: Time for loops' % ((time.time() - start_time) / 60))\n",
        "\n",
        "\n",
        "# Or same in \"compact\" manner\n",
        "LAG_DAYS = [col for col in range(1,8)]\n",
        "temp_df = train_df[['id','d',TARGET]]\n",
        "\n",
        "start_time = time.time()\n",
        "temp_df = temp_df.assign(**{\n",
        "        '{}_lag_{}'.format(col, l): temp_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
        "        for l in LAG_DAYS\n",
        "        for col in [TARGET]\n",
        "    })\n",
        "\n",
        "print('%0.2f min: Time for bulk shift' % ((time.time() - start_time) / 60))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shifting: 1\n",
            "Shifting: 2\n",
            "Shifting: 3\n",
            "Shifting: 4\n",
            "Shifting: 5\n",
            "Shifting: 6\n",
            "Shifting: 7\n",
            "0.16 min: Time for loops\n",
            "0.16 min: Time for bulk shift\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_p94itSNEA3",
        "outputId": "4429ca63-5882-4613-9fbc-c790076e4934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "temp_df"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>sales_lag_1</th>\n",
              "      <th>sales_lag_2</th>\n",
              "      <th>sales_lag_3</th>\n",
              "      <th>sales_lag_4</th>\n",
              "      <th>sales_lag_5</th>\n",
              "      <th>sales_lag_6</th>\n",
              "      <th>sales_lag_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_014_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_056_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_062_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333077</th>\n",
              "      <td>FOODS_3_807_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333078</th>\n",
              "      <td>FOODS_3_810_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333079</th>\n",
              "      <td>FOODS_3_814_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333080</th>\n",
              "      <td>FOODS_3_815_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333081</th>\n",
              "      <td>FOODS_3_817_CA_4_validation</td>\n",
              "      <td>d_1941</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2333082 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    id       d  ...  sales_lag_6  sales_lag_7\n",
              "0        HOBBIES_1_010_CA_1_validation     d_1  ...          NaN          NaN\n",
              "1        HOBBIES_1_012_CA_1_validation     d_1  ...          NaN          NaN\n",
              "2        HOBBIES_1_014_CA_1_validation     d_1  ...          NaN          NaN\n",
              "3        HOBBIES_1_056_CA_1_validation     d_1  ...          NaN          NaN\n",
              "4        HOBBIES_1_062_CA_1_validation     d_1  ...          NaN          NaN\n",
              "...                                ...     ...  ...          ...          ...\n",
              "2333077    FOODS_3_807_CA_4_validation  d_1941  ...          4.0          0.0\n",
              "2333078    FOODS_3_810_CA_4_validation  d_1941  ...          2.0          5.0\n",
              "2333079    FOODS_3_814_CA_4_validation  d_1941  ...          1.0          1.0\n",
              "2333080    FOODS_3_815_CA_4_validation  d_1941  ...          0.0          0.0\n",
              "2333081    FOODS_3_817_CA_4_validation  d_1941  ...          0.0          0.0\n",
              "\n",
              "[2333082 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm-M0n-5NJWE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GRKdtheNMjF"
      },
      "source": [
        "***We can notice many NaNs values - which is normal because there is no data for day 0,-1,-2 (out of dataset time periods)\n",
        "\n",
        "Same works for test set\n",
        "\n",
        "be careful to make lag features:\n",
        "\n",
        "*for day 1920 there is no data about day 1919 (until 1913)\n",
        "So if we want to predict day 1915, our lag features have to start from 2 (1915 (which is the predicting day)-1913 (the last day with label in dataset), and so on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiLHPbzVNNJX",
        "outputId": "3e2f574f-b61d-4520-e114-ec5108178d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "# The result\n",
        "temp_df[temp_df['id']=='HOBBIES_1_010_CA_1_validation'].iloc[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>sales_lag_1</th>\n",
              "      <th>sales_lag_2</th>\n",
              "      <th>sales_lag_3</th>\n",
              "      <th>sales_lag_4</th>\n",
              "      <th>sales_lag_5</th>\n",
              "      <th>sales_lag_6</th>\n",
              "      <th>sales_lag_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3606</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4808</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6010</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_6</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7212</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_7</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_8</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9616</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10818</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>d_10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id     d  ...  sales_lag_6  sales_lag_7\n",
              "0      HOBBIES_1_010_CA_1_validation   d_1  ...          NaN          NaN\n",
              "1202   HOBBIES_1_010_CA_1_validation   d_2  ...          NaN          NaN\n",
              "2404   HOBBIES_1_010_CA_1_validation   d_3  ...          NaN          NaN\n",
              "3606   HOBBIES_1_010_CA_1_validation   d_4  ...          NaN          NaN\n",
              "4808   HOBBIES_1_010_CA_1_validation   d_5  ...          NaN          NaN\n",
              "6010   HOBBIES_1_010_CA_1_validation   d_6  ...          NaN          NaN\n",
              "7212   HOBBIES_1_010_CA_1_validation   d_7  ...          0.0          NaN\n",
              "8414   HOBBIES_1_010_CA_1_validation   d_8  ...          0.0          0.0\n",
              "9616   HOBBIES_1_010_CA_1_validation   d_9  ...          1.0          0.0\n",
              "10818  HOBBIES_1_010_CA_1_validation  d_10  ...          0.0          1.0\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3NVzULINN7g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXV9eJZBNUFy"
      },
      "source": [
        "\n",
        "Rolling lags\n",
        "\n",
        "Because of computational limitation, we restored few days of sales values from horizontal representation as lag features\n",
        "\n",
        "*lambda x: x.shift(1)==> 1 day shift will serve only to predict day 1914\n",
        "\n",
        "for other days we have to shift PREDICT_DAY-1913\n",
        "\n",
        "Lets go for rolling aggregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZsA7yXKNOBw",
        "outputId": "3098fa2b-5253-43d5-9b15-5a23d7bd7459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "########################### Rolling lags ###########################################################\n",
        "\n",
        "## Rolling aggregations\n",
        "\n",
        "\n",
        "\n",
        "# Such aggregations will help us to restore\n",
        "# at least part of the information for our model\n",
        "\n",
        "temp_df = train_df[['id','d','sales']]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i in [14,30,60]:\n",
        "    print('Rolling period:', i)\n",
        "    temp_df['rolling_mean_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
        "    temp_df['rolling_std_'+str(i)]  = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
        "\n",
        "# we can also aggregate by max/skew/median etc \n",
        "\n",
        "print('%0.2f min: Time for loop' % ((time.time() - start_time) / 60))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rolling period: 14\n",
            "Rolling period: 30\n",
            "Rolling period: 60\n",
            "0.20 min: Time for loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlHm8VSKNOHw",
        "outputId": "2f44f9fb-04ab-44b7-c549-b77850a892d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "# The result \n",
        "temp_df[temp_df['id']=='HOBBIES_1_012_CA_1_validation'].iloc[:20]\n",
        "\n",
        "# Same as before, the NaNs values - is is because of absence of data for \n",
        "# 0*(rolling_period),-1*(rolling_period),-2*(rolling_period)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>rolling_mean_14</th>\n",
              "      <th>rolling_std_14</th>\n",
              "      <th>rolling_mean_30</th>\n",
              "      <th>rolling_std_30</th>\n",
              "      <th>rolling_mean_60</th>\n",
              "      <th>rolling_std_60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_3</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3607</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_4</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4809</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6011</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_6</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7213</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_7</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8415</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_8</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9617</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_9</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10819</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_10</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12021</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_11</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13223</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_12</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14425</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_13</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15627</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_14</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16829</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_15</td>\n",
              "      <td>0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18031</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_16</td>\n",
              "      <td>2</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19233</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_17</td>\n",
              "      <td>0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20435</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_18</td>\n",
              "      <td>3</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.851631</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21637</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_19</td>\n",
              "      <td>0</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>1.081818</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22839</th>\n",
              "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
              "      <td>d_20</td>\n",
              "      <td>1</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>1.081818</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  id     d  ...  rolling_mean_60  rolling_std_60\n",
              "1      HOBBIES_1_012_CA_1_validation   d_1  ...              NaN             NaN\n",
              "1203   HOBBIES_1_012_CA_1_validation   d_2  ...              NaN             NaN\n",
              "2405   HOBBIES_1_012_CA_1_validation   d_3  ...              NaN             NaN\n",
              "3607   HOBBIES_1_012_CA_1_validation   d_4  ...              NaN             NaN\n",
              "4809   HOBBIES_1_012_CA_1_validation   d_5  ...              NaN             NaN\n",
              "6011   HOBBIES_1_012_CA_1_validation   d_6  ...              NaN             NaN\n",
              "7213   HOBBIES_1_012_CA_1_validation   d_7  ...              NaN             NaN\n",
              "8415   HOBBIES_1_012_CA_1_validation   d_8  ...              NaN             NaN\n",
              "9617   HOBBIES_1_012_CA_1_validation   d_9  ...              NaN             NaN\n",
              "10819  HOBBIES_1_012_CA_1_validation  d_10  ...              NaN             NaN\n",
              "12021  HOBBIES_1_012_CA_1_validation  d_11  ...              NaN             NaN\n",
              "13223  HOBBIES_1_012_CA_1_validation  d_12  ...              NaN             NaN\n",
              "14425  HOBBIES_1_012_CA_1_validation  d_13  ...              NaN             NaN\n",
              "15627  HOBBIES_1_012_CA_1_validation  d_14  ...              NaN             NaN\n",
              "16829  HOBBIES_1_012_CA_1_validation  d_15  ...              NaN             NaN\n",
              "18031  HOBBIES_1_012_CA_1_validation  d_16  ...              NaN             NaN\n",
              "19233  HOBBIES_1_012_CA_1_validation  d_17  ...              NaN             NaN\n",
              "20435  HOBBIES_1_012_CA_1_validation  d_18  ...              NaN             NaN\n",
              "21637  HOBBIES_1_012_CA_1_validation  d_19  ...              NaN             NaN\n",
              "22839  HOBBIES_1_012_CA_1_validation  d_20  ...              NaN             NaN\n",
              "\n",
              "[20 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A9t9VjzNd88"
      },
      "source": [
        "'Memory usage' minify mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st9rbBgQNOOU",
        "outputId": "1d1a5e88-1295-41b1-b31c-77e83805c3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "########################### Memory ussage ###################################################\n",
        "# Let's check our memory usage\n",
        "print(\"{:>20}: {:>8}\".format('Original rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n",
        "\n",
        "# can we minify it?\n",
        "# 1. if our dataset are aligned by index \n",
        "#    we don't need 'id' 'd' 'sales' columns\n",
        "temp_df = temp_df.iloc[:,3:]\n",
        "print(\"{:>20}: {:>8}\".format('Values rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n",
        "\n",
        "# can we make it even smaller?\n",
        "# yes, carefully change dtype and/or use sparce matrix to minify 0s\n",
        "# Also note that lgbm accepts matrixes as input that is good for memory reducion \n",
        "from scipy import sparse \n",
        "temp_matrix = sparse.csr_matrix(temp_df)\n",
        "\n",
        "# restore to df\n",
        "temp_matrix_restored = pd.DataFrame(temp_matrix.todense())\n",
        "restored_cols = ['roll_' + str(i) for i in list(temp_matrix_restored)]\n",
        "temp_matrix_restored.columns = restored_cols"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original rolling df: 160.2MiB\n",
            "   Values rolling df: 106.8MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr2TRIaiNOYT"
      },
      "source": [
        "########################### Remove old objects\n",
        "#################################################################################\n",
        "del temp_df, train_df, temp_matrix, temp_matrix_restored"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeesrjwENOfI",
        "outputId": "600ceb6b-5d60-42a4-a27a-c30c0d5a6496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "########################### Apply on grid_df #####################################################\n",
        "# lets read first grid from \n",
        "grid_df = pd.read_pickle('/content/drive/My Drive/GoogleColab/pickle_files/grid_part_1.pkl')\n",
        "\n",
        "# We need only 'id','d','sales'\n",
        "# to make lags and rollings\n",
        "grid_df = grid_df[['id','d','sales']]\n",
        "SHIFT_DAY = 28\n",
        "\n",
        "# Lags\n",
        "# with 28 day shift\n",
        "start_time = time.time()\n",
        "print('Create lags')\n",
        "\n",
        "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n",
        "grid_df = grid_df.assign(**{\n",
        "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
        "        for l in LAG_DAYS\n",
        "        for col in [TARGET]\n",
        "    })\n",
        "\n",
        "# Minify lag columns\n",
        "for col in list(grid_df):\n",
        "    if 'lag' in col:\n",
        "        grid_df[col] = grid_df[col].astype(np.float16)\n",
        "\n",
        "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
        "\n",
        "# Rollings\n",
        "# with 28 day shift\n",
        "start_time = time.time()\n",
        "print('Create rolling aggs')\n",
        "\n",
        "for i in [7,14,30,60,180]:\n",
        "    print('Rolling period:', i)\n",
        "    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n",
        "    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n",
        "\n",
        "# Rollings\n",
        "# with sliding shift\n",
        "for d_shift in [1,7,14]: \n",
        "    print('Shifting period:', d_shift)\n",
        "    for d_window in [7,14,30,60]:\n",
        "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
        "        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
        "    \n",
        "    \n",
        "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create lags\n",
            "0.57 min: Lags\n",
            "Create rolling aggs\n",
            "Rolling period: 7\n",
            "Rolling period: 14\n",
            "Rolling period: 30\n",
            "Rolling period: 60\n",
            "Rolling period: 180\n",
            "Shifting period: 1\n",
            "Shifting period: 7\n",
            "Shifting period: 14\n",
            "1.38 min: Lags\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8a6hdNONOpC",
        "outputId": "0e8a7608-3ed6-48cd-fa26-0109558ecc47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "########################## Export ###############################################\n",
        "path='/content/drive/My Drive/GoogleColab/pickle_files/'\n",
        "print('Save lags and rollings')\n",
        "grid_df.to_pickle(path+'lags_df_'+str(SHIFT_DAY)+'.pkl')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save lags and rollings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7E11Z9GNOvZ",
        "outputId": "39771cc4-88cb-4036-81c6-db0e64801e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "########################### Final list of new features ################################################\n",
        "grid_df.info()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 40 columns):\n",
            " #   Column                  Dtype   \n",
            "---  ------                  -----   \n",
            " 0   id                      category\n",
            " 1   d                       int16   \n",
            " 2   sales                   float64 \n",
            " 3   sales_lag_28            float16 \n",
            " 4   sales_lag_29            float16 \n",
            " 5   sales_lag_30            float16 \n",
            " 6   sales_lag_31            float16 \n",
            " 7   sales_lag_32            float16 \n",
            " 8   sales_lag_33            float16 \n",
            " 9   sales_lag_34            float16 \n",
            " 10  sales_lag_35            float16 \n",
            " 11  sales_lag_36            float16 \n",
            " 12  sales_lag_37            float16 \n",
            " 13  sales_lag_38            float16 \n",
            " 14  sales_lag_39            float16 \n",
            " 15  sales_lag_40            float16 \n",
            " 16  sales_lag_41            float16 \n",
            " 17  sales_lag_42            float16 \n",
            " 18  rolling_mean_7          float16 \n",
            " 19  rolling_std_7           float16 \n",
            " 20  rolling_mean_14         float16 \n",
            " 21  rolling_std_14          float16 \n",
            " 22  rolling_mean_30         float16 \n",
            " 23  rolling_std_30          float16 \n",
            " 24  rolling_mean_60         float16 \n",
            " 25  rolling_std_60          float16 \n",
            " 26  rolling_mean_180        float16 \n",
            " 27  rolling_std_180         float16 \n",
            " 28  rolling_mean_tmp_1_7    float16 \n",
            " 29  rolling_mean_tmp_1_14   float16 \n",
            " 30  rolling_mean_tmp_1_30   float16 \n",
            " 31  rolling_mean_tmp_1_60   float16 \n",
            " 32  rolling_mean_tmp_7_7    float16 \n",
            " 33  rolling_mean_tmp_7_14   float16 \n",
            " 34  rolling_mean_tmp_7_30   float16 \n",
            " 35  rolling_mean_tmp_7_60   float16 \n",
            " 36  rolling_mean_tmp_14_7   float16 \n",
            " 37  rolling_mean_tmp_14_14  float16 \n",
            " 38  rolling_mean_tmp_14_30  float16 \n",
            " 39  rolling_mean_tmp_14_60  float16 \n",
            "dtypes: category(1), float16(37), float64(1), int16(1)\n",
            "memory usage: 375.6 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKa_ucgWNywk"
      },
      "source": [
        "Part 3: Mean encoding (Custom encoding)\n",
        "FE creation approaches\n",
        "Sequential FE validation\n",
        "Dimension reduction\n",
        "FE validation by Permutation importance\n",
        "Mean encodings\n",
        "Parallelization for FE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiPOf5rANO00"
      },
      "source": [
        "#read data\n",
        "#Lets read same 3 sets of features\n",
        "path='/content/drive/My Drive/GoogleColab/pickle_files/'\n",
        "grid_df = pd.concat([pd.read_pickle(path+'grid_part_1.pkl'),\n",
        "                     pd.read_pickle(path+'grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle(path+'grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        " ## OR\n",
        "#grid_df=pd.concat([grid_part_1, grid_part_2.iloc[:,2:], grid_part_3.iloc[:,2:]], axis=1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXqnreWNNOl2",
        "outputId": "20eef814-a67d-4698-ef82-569f11fb5b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "grid_df.info()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 35 columns):\n",
            " #   Column            Dtype   \n",
            "---  ------            -----   \n",
            " 0   id                category\n",
            " 1   item_id           category\n",
            " 2   dept_id           category\n",
            " 3   cat_id            category\n",
            " 4   store_id          category\n",
            " 5   state_id          category\n",
            " 6   d                 int16   \n",
            " 7   sales             float64 \n",
            " 8   release           int16   \n",
            " 9   wm_yr_wk          int64   \n",
            " 10  sell_price        float16 \n",
            " 11  price_max         float16 \n",
            " 12  price_min         float16 \n",
            " 13  price_std         float16 \n",
            " 14  price_mean        float16 \n",
            " 15  price_norm        float16 \n",
            " 16  price_nunique     float16 \n",
            " 17  item_nunique      float16 \n",
            " 18  price_momentum    float16 \n",
            " 19  price_momentum_m  float16 \n",
            " 20  price_momentum_y  float16 \n",
            " 21  event_name_1      category\n",
            " 22  event_type_1      category\n",
            " 23  event_name_2      category\n",
            " 24  event_type_2      category\n",
            " 25  snap_CA           category\n",
            " 26  snap_TX           category\n",
            " 27  snap_WI           category\n",
            " 28  tm_d              int8    \n",
            " 29  tm_w              int8    \n",
            " 30  tm_m              int8    \n",
            " 31  tm_y              int8    \n",
            " 32  tm_wm             int8    \n",
            " 33  tm_dw             int8    \n",
            " 34  tm_w_end          int8    \n",
            "dtypes: category(13), float16(11), float64(1), int16(2), int64(1), int8(7)\n",
            "memory usage: 279.7 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnqaxkN3NOVI",
        "outputId": "293ff4ab-7150-47f1-ac19-9f2da8bf3887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "# Subsampling\n",
        "# to make all calculations faster.\n",
        "# Keep only 5% of original ids.\n",
        "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
        "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
        "\n",
        "# Let's \"inspect\" our grid DataFrame\n",
        "grid_df.info()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 300865 entries, 0 to 300864\n",
            "Data columns (total 35 columns):\n",
            " #   Column            Non-Null Count   Dtype   \n",
            "---  ------            --------------   -----   \n",
            " 0   id                300865 non-null  category\n",
            " 1   item_id           300865 non-null  category\n",
            " 2   dept_id           300865 non-null  category\n",
            " 3   cat_id            300865 non-null  category\n",
            " 4   store_id          300865 non-null  category\n",
            " 5   state_id          300865 non-null  category\n",
            " 6   d                 300865 non-null  int16   \n",
            " 7   sales             296581 non-null  float64 \n",
            " 8   release           300865 non-null  int16   \n",
            " 9   wm_yr_wk          300865 non-null  int64   \n",
            " 10  sell_price        30956 non-null   float16 \n",
            " 11  price_max         30956 non-null   float16 \n",
            " 12  price_min         30956 non-null   float16 \n",
            " 13  price_std         30956 non-null   float16 \n",
            " 14  price_mean        30956 non-null   float16 \n",
            " 15  price_norm        30956 non-null   float16 \n",
            " 16  price_nunique     30956 non-null   float16 \n",
            " 17  item_nunique      30956 non-null   float16 \n",
            " 18  price_momentum    29885 non-null   float16 \n",
            " 19  price_momentum_m  30956 non-null   float16 \n",
            " 20  price_momentum_y  30956 non-null   float16 \n",
            " 21  event_name_1      24786 non-null   category\n",
            " 22  event_type_1      24786 non-null   category\n",
            " 23  event_name_2      612 non-null     category\n",
            " 24  event_type_2      612 non-null     category\n",
            " 25  snap_CA           300865 non-null  category\n",
            " 26  snap_TX           300865 non-null  category\n",
            " 27  snap_WI           300865 non-null  category\n",
            " 28  tm_d              300865 non-null  int8    \n",
            " 29  tm_w              300865 non-null  int8    \n",
            " 30  tm_m              300865 non-null  int8    \n",
            " 31  tm_y              300865 non-null  int8    \n",
            " 32  tm_wm             300865 non-null  int8    \n",
            " 33  tm_dw             300865 non-null  int8    \n",
            " 34  tm_w_end          300865 non-null  int8    \n",
            "dtypes: category(13), float16(11), float64(1), int16(2), int64(1), int8(7)\n",
            "memory usage: 18.6 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C69sa-pXN_iR",
        "outputId": "56f19bfd-da14-40f8-9ac1-46ae8f73244f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "grid_df.head(2)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>dept_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>store_id</th>\n",
              "      <th>state_id</th>\n",
              "      <th>d</th>\n",
              "      <th>sales</th>\n",
              "      <th>release</th>\n",
              "      <th>wm_yr_wk</th>\n",
              "      <th>sell_price</th>\n",
              "      <th>price_max</th>\n",
              "      <th>price_min</th>\n",
              "      <th>price_std</th>\n",
              "      <th>price_mean</th>\n",
              "      <th>price_norm</th>\n",
              "      <th>price_nunique</th>\n",
              "      <th>item_nunique</th>\n",
              "      <th>price_momentum</th>\n",
              "      <th>price_momentum_m</th>\n",
              "      <th>price_momentum_y</th>\n",
              "      <th>event_name_1</th>\n",
              "      <th>event_type_1</th>\n",
              "      <th>event_name_2</th>\n",
              "      <th>event_type_2</th>\n",
              "      <th>snap_CA</th>\n",
              "      <th>snap_TX</th>\n",
              "      <th>snap_WI</th>\n",
              "      <th>tm_d</th>\n",
              "      <th>tm_w</th>\n",
              "      <th>tm_m</th>\n",
              "      <th>tm_y</th>\n",
              "      <th>tm_wm</th>\n",
              "      <th>tm_dw</th>\n",
              "      <th>tm_w_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_303_CA_1_validation</td>\n",
              "      <td>HOBBIES_1_303</td>\n",
              "      <td>HOBBIES_1</td>\n",
              "      <td>HOBBIES</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>11101</td>\n",
              "      <td>8.976562</td>\n",
              "      <td>8.976562</td>\n",
              "      <td>8.562500</td>\n",
              "      <td>0.175903</td>\n",
              "      <td>8.890625</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOUSEHOLD_1_401_CA_1_validation</td>\n",
              "      <td>HOUSEHOLD_1_401</td>\n",
              "      <td>HOUSEHOLD_1</td>\n",
              "      <td>HOUSEHOLD</td>\n",
              "      <td>CA_1</td>\n",
              "      <td>CA</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>11101</td>\n",
              "      <td>8.867188</td>\n",
              "      <td>9.468750</td>\n",
              "      <td>8.867188</td>\n",
              "      <td>0.266846</td>\n",
              "      <td>9.312500</td>\n",
              "      <td>0.936523</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.967285</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                id          item_id  ... tm_dw tm_w_end\n",
              "0    HOBBIES_1_303_CA_1_validation    HOBBIES_1_303  ...     5        1\n",
              "1  HOUSEHOLD_1_401_CA_1_validation  HOUSEHOLD_1_401  ...     5        1\n",
              "\n",
              "[2 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSKTLbJ_OoU9"
      },
      "source": [
        "#Part 3.1 Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRr5wWkON_ti"
      },
      "source": [
        "########################### Baseline model ##########################################\n",
        "\n",
        "# We will need some global VARS for future\n",
        "\n",
        "SEED = 142             # Our random seed for everything\n",
        "random.seed(SEED)      # to make all tests \"deterministic\"\n",
        "np.random.seed(SEED)\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores (we have just 2)\n",
        "\n",
        "TARGET = 'sales'      # Our Target\n",
        "END_TRAIN = 1913      # And we will use last 28 days as validation\n",
        "\n",
        "# Drop some items from \"TEST\" set part (1914...)\n",
        "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
        "\n",
        "# Features that we want to exclude from training\n",
        "remove_features = ['id','d',TARGET]\n",
        "\n",
        "# Our baseline model serves to do fast checks of new features performance \n",
        "\n",
        "# We will use LightGBM for our tests\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
        "                    'objective': 'regression',       # Standart loss for RMSE\n",
        "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
        "                    'subsample': 0.8,                \n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
        "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
        "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
        "                    'feature_fraction': 0.8,\n",
        "                    'n_estimators': 5000,            # We don't want to limit training (we can change 5000 to any big enough number)\n",
        "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
        "                    'seed': SEED,\n",
        "                    'verbose': -1,\n",
        "                }"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDjMQjTrN_4u",
        "outputId": "d6e42021-5da2-4207-b55c-add9bf225579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "## RMSE\n",
        "def rmse(y, y_pred):\n",
        "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
        "\n",
        "# Small function to make fast features tests\n",
        "# estimator = make_fast_test(grid_df)\n",
        "# it will return lgb booster for future analisys\n",
        "def make_fast_test(df):\n",
        "\n",
        "    features_columns = [col for col in list(df) if col not in remove_features]\n",
        "\n",
        "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n",
        "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
        "    \n",
        "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
        "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
        "    \n",
        "    estimator = lgb.train(\n",
        "                            lgb_params,\n",
        "                            train_data,\n",
        "                            valid_sets = [train_data,valid_data],\n",
        "                            verbose_eval = 500,\n",
        "                        )\n",
        "    \n",
        "    return estimator\n",
        "\n",
        "# Make baseline model\n",
        "baseline_model = make_fast_test(grid_df)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[336]\ttraining's rmse: 1.86888\tvalid_1's rmse: 1.85155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drqwezLwOyzX",
        "outputId": "6685e630-83de-48d5-b12a-4612f571baf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Lets test our normal Lags (7 days)###########################\n",
        "\n",
        "# Small helper to make lags creation faster\n",
        "from multiprocessing import Pool                # Multiprocess Runs\n",
        "\n",
        "## Multiprocessing Run.\n",
        "# :t_split - int of lags days                   # type: int\n",
        "# :func - Function to apply on each split       # type: python function\n",
        "\n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "def make_normal_lag(lag_day):\n",
        "    lag_df = grid_df[['id','d',TARGET]] \n",
        "    col_name = 'sales_lag_'+str(lag_day)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "# Launch parallel lag creation\n",
        "# and \"append\" to our grid\n",
        "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[325]\ttraining's rmse: 1.72333\tvalid_1's rmse: 1.75514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-LWnq63O3Fz",
        "outputId": "190de807-607e-46a3-d64c-335b81e2c539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "########################### Permutation importance Test\n",
        "########################### https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "#################################################################################\n",
        "\n",
        "# Let's creat validation dataset and features\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "\n",
        "# Make normal prediction with our model and save score\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "\n",
        "# Now we are looping over all our numerical features\n",
        "for col in features_columns:\n",
        "    \n",
        "    # We will make validation set copy to restore\n",
        "    # features states on each run\n",
        "    temp_df = validation_df.copy()\n",
        "    \n",
        "    # Error here appears if we have \"categorical\" features and can't \n",
        "    # do np.random.permutation without disrupt categories\n",
        "    # so we need to check if feature is numerical\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        \n",
        "        # If our current rmse score is less than base score\n",
        "        # it means that feature most probably is a bad one\n",
        "        # and our model is learning on noise\n",
        "        print(col, np.round(cur_score - base_score, 4))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standart RMSE 1.7551410666872997\n",
            "release -0.0002\n",
            "wm_yr_wk 0.0\n",
            "sell_price -0.0003\n",
            "price_max 0.0001\n",
            "price_min 0.0001\n",
            "price_std -0.0001\n",
            "price_mean -0.0002\n",
            "price_norm -0.0002\n",
            "price_nunique 0.0\n",
            "item_nunique 0.0003\n",
            "price_momentum -0.0\n",
            "price_momentum_m 0.0004\n",
            "price_momentum_y 0.0\n",
            "tm_d 0.0081\n",
            "tm_w 0.0004\n",
            "tm_m -0.0003\n",
            "tm_y 0.0\n",
            "tm_wm 0.0004\n",
            "tm_dw 0.0886\n",
            "tm_w_end 0.0143\n",
            "sales_lag_1 0.3169\n",
            "sales_lag_2 0.0245\n",
            "sales_lag_3 0.0435\n",
            "sales_lag_4 0.0344\n",
            "sales_lag_5 0.0104\n",
            "sales_lag_6 0.0018\n",
            "sales_lag_7 0.0428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mS3_7HDO-m7"
      },
      "source": [
        "\n",
        "Insights:\n",
        "Lags with 1 days shift (nearest past) shows the important features while other features not much.\n",
        "\n",
        "Better to test with several Permutation for confirmation (https://www.kaggle.com/dansbecker/permutation-importance @dansbecker)\n",
        "\n",
        "price_nunique -0.002 : strong negative values are most probably noise\n",
        "\n",
        "The idea is the following: feature importance can be measured by looking at how much the score (accuracy, mse, rmse, mae, etc. - any score we’re interested in) decreases when a feature is not available.\n",
        "\n",
        "To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n",
        "\n",
        "To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature.\n",
        "\n",
        "Instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information.\n",
        "\n",
        "This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9H6RbUSO3Qc"
      },
      "source": [
        "# Remove Temp data\n",
        "del temp_df, validation_df\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dkfmBOePC7T",
        "outputId": "2c7ecf10-16d7-43a4-fc8d-dfa3b59473c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "for col in features_columns:\n",
        "    temp_df = validation_df.copy()\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "del temp_df, validation_df"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[159]\ttraining's rmse: 1.98114\tvalid_1's rmse: 1.86107\n",
            "Standart RMSE 1.8610747804749486\n",
            "release 0.0001\n",
            "wm_yr_wk 0.0\n",
            "sell_price 0.0002\n",
            "price_max 0.0004\n",
            "price_min -0.0001\n",
            "price_std 0.0\n",
            "price_mean 0.0002\n",
            "price_norm 0.0001\n",
            "price_nunique 0.0\n",
            "item_nunique -0.0003\n",
            "price_momentum -0.0002\n",
            "price_momentum_m -0.0008\n",
            "price_momentum_y -0.0001\n",
            "tm_d 0.0077\n",
            "tm_w 0.0004\n",
            "tm_m 0.0004\n",
            "tm_y 0.0\n",
            "tm_wm 0.0005\n",
            "tm_dw 0.0538\n",
            "tm_w_end 0.0175\n",
            "sales_lag_56 0.0366\n",
            "sales_lag_57 0.0296\n",
            "sales_lag_58 0.0105\n",
            "sales_lag_59 0.0163\n",
            "sales_lag_60 0.0142\n",
            "sales_lag_61 0.0114\n",
            "sales_lag_62 0.0219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNJZIK7AO3Zv"
      },
      "source": [
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 56 days shift (far away past) are not as important\n",
        "## as nearest past lags\n",
        "## and at some point will be just noise for our model"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lftWnE2dPIap"
      },
      "source": [
        "\n",
        "Part 3.2 Implementation of PCA algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjr4JBtKO3n8"
      },
      "source": [
        "########################### PCA #################################################\n",
        "\n",
        "# The main question here - can we have almost same rmse boost with less features\n",
        "# less dimensionality?\n",
        "\n",
        "# Lets try PCA and make 7->3 dimensionality reduction\n",
        "\n",
        "# PCA is \"unsupervised\" learning and with shifted target we can be sure\n",
        "# that we have no Target leakage\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def make_pca(df, pca_col, n_days):\n",
        "    print('PCA:', pca_col, n_days)\n",
        "    \n",
        "    # We don't need any other columns to make pca\n",
        "    pca_df = df[[pca_col,'d',TARGET]]\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\" \n",
        "    # we need to agg first\n",
        "    if pca_col != 'id':\n",
        "        merge_base = pca_df[[pca_col,'d']]\n",
        "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
        "        pca_df[TARGET] = pca_df['sum']\n",
        "        del pca_df['sum']\n",
        "    \n",
        "    # Min/Max scaling\n",
        "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
        "    \n",
        "    # Making \"lag\" in old way (not parallel)\n",
        "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
        "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
        "    pca_df = pca_df.assign(**{\n",
        "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
        "            for l in LAG_DAYS\n",
        "            for col in [TARGET]\n",
        "        })\n",
        "    \n",
        "    pca_columns = list(pca_df)[3:]\n",
        "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
        "    pca = PCA(random_state=SEED)\n",
        "    \n",
        "    # You can use fit_transform here\n",
        "    pca.fit(pca_df[pca_columns])\n",
        "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
        "    \n",
        "    print(pca.explained_variance_ratio_)\n",
        "    \n",
        "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
        "    keep_cols = pca_columns[:3]\n",
        "    print('Columns to keep:', keep_cols)\n",
        "    \n",
        "    # for simple explanation: \n",
        "    # https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "    \n",
        "    return pca_df[keep_cols]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ_hwT1fPRvY",
        "outputId": "4a570fa2-f5c9-42f8-e40f-e6ee40d29ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Make PCA\n",
        "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA: id 7\n",
            "[0.71783945 0.06713417 0.05399105 0.04415797 0.04118687 0.03797171\n",
            " 0.03771877]\n",
            "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[105]\ttraining's rmse: 1.87741\tvalid_1's rmse: 1.78522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl3jUYE0PWN4",
        "outputId": "93bb2ed1-2d6f-4ae7-e15a-2f8b60d2cbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################### Mean/std target encoding ################################################\n",
        "\n",
        "# We will use these three columns for test\n",
        "# (in combination with store_id)\n",
        "icols = ['item_id','cat_id','dept_id']\n",
        "\n",
        "# But we can use any other column or even multiple groups\n",
        "# like these ones\n",
        "#            'state_id',\n",
        "#            'store_id',\n",
        "#            'cat_id',\n",
        "#            'dept_id',\n",
        "#            ['state_id', 'cat_id'],\n",
        "#            ['state_id', 'dept_id'],\n",
        "#            ['store_id', 'cat_id'],\n",
        "#            ['store_id', 'dept_id'],\n",
        "#            'item_id',\n",
        "#            ['item_id', 'state_id'],\n",
        "#            ['item_id', 'store_id']\n",
        "\n",
        "# There are several ways to do \"mean\" encoding\n",
        "## K-fold scheme\n",
        "## LOO (leave one out)\n",
        "## Smoothed/regularized \n",
        "## Expanding mean\n",
        "## etc \n",
        "\n",
        "# We will use simple target encoding\n",
        "# by std and mean agg\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n",
        "    \n",
        "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
        "    joiner = '_'+col+'_encoding_'\n",
        "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
        "    temp_df = temp_df.reset_index()\n",
        "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
        "    del temp_df\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding item_id\n",
            "Encoding cat_id\n",
            "Encoding dept_id\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvBG_IctPZCD",
        "outputId": "460522b4-8bc0-4348-8354-8229595223fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "# Bad thing that for some items  \n",
        "# we are using past and future values.\n",
        "# But we are looking for \"categorical\" similiarity\n",
        "# on a \"long run\". So future here is not a big problem."
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[237]\ttraining's rmse: 1.90614\tvalid_1's rmse: 1.8554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6tBmBBmPct0"
      },
      "source": [
        "\n",
        "########################### Last Non 0 sale ##########################\n",
        "\n",
        "def find_last_sale(df,n_day):\n",
        "    \n",
        "    # Limit initial df\n",
        "    ls_df = df[['id','d',TARGET]]\n",
        "    \n",
        "    # Convert target to binary\n",
        "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
        "    \n",
        "    # Make lags to prevent any leakage\n",
        "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
        "\n",
        "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
        "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
        "\n",
        "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
        "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
        "\n",
        "    return ls_df[['last_sale']]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecI9kTf5PknL",
        "outputId": "65a4ccda-e8e3-4495-f46c-a1e7f1681a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Find last non zero\n",
        "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "########################### Apply on grid_df ###################################################\n",
        "# lets read grid from \n",
        "grid_df = pd.read_pickle(path+'grid_part_1.pkl')\n",
        "#grid_df=grid_part_1.copy()\n",
        "#grid_df['d']=grid_df['d'].str[2:6].astype(int) #need to change 'd' into numeric\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[242]\ttraining's rmse: 1.85178\tvalid_1's rmse: 1.79173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oyq0ObxPpaU",
        "outputId": "de1d42e2-8187-4f9a-95b4-cbbaa5966e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "\n",
        "icols =  [\n",
        "            ['state_id'],\n",
        "            ['store_id'],\n",
        "            ['cat_id'],\n",
        "            ['dept_id'],\n",
        "            ['state_id', 'cat_id'],\n",
        "            ['state_id', 'dept_id'],\n",
        "            ['store_id', 'cat_id'],\n",
        "            ['store_id', 'dept_id'],\n",
        "            ['item_id'],\n",
        "            ['item_id', 'state_id'],\n",
        "            ['item_id', 'store_id']\n",
        "            ]\n",
        "\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    col_name = '_'+'_'.join(col)+'_'\n",
        "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
        "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]\n",
        "\n",
        "# #################################################################################\n",
        "print('Save Mean/Std encoding')\n",
        "grid_df.to_pickle(path+'mean_encoding_df.pkl')\n",
        "\n",
        "########################### Final list of new features\n",
        "#################################################################################\n",
        "grid_df.info()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding ['state_id']\n",
            "Encoding ['store_id']\n",
            "Encoding ['cat_id']\n",
            "Encoding ['dept_id']\n",
            "Encoding ['state_id', 'cat_id']\n",
            "Encoding ['state_id', 'dept_id']\n",
            "Encoding ['store_id', 'cat_id']\n",
            "Encoding ['store_id', 'dept_id']\n",
            "Encoding ['item_id']\n",
            "Encoding ['item_id', 'state_id']\n",
            "Encoding ['item_id', 'store_id']\n",
            "Save Mean/Std encoding\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4578794 entries, 0 to 4578793\n",
            "Data columns (total 24 columns):\n",
            " #   Column                     Dtype   \n",
            "---  ------                     -----   \n",
            " 0   id                         category\n",
            " 1   d                          int16   \n",
            " 2   enc_state_id_mean          float16 \n",
            " 3   enc_state_id_std           float16 \n",
            " 4   enc_store_id_mean          float16 \n",
            " 5   enc_store_id_std           float16 \n",
            " 6   enc_cat_id_mean            float16 \n",
            " 7   enc_cat_id_std             float16 \n",
            " 8   enc_dept_id_mean           float16 \n",
            " 9   enc_dept_id_std            float16 \n",
            " 10  enc_state_id_cat_id_mean   float16 \n",
            " 11  enc_state_id_cat_id_std    float16 \n",
            " 12  enc_state_id_dept_id_mean  float16 \n",
            " 13  enc_state_id_dept_id_std   float16 \n",
            " 14  enc_store_id_cat_id_mean   float16 \n",
            " 15  enc_store_id_cat_id_std    float16 \n",
            " 16  enc_store_id_dept_id_mean  float16 \n",
            " 17  enc_store_id_dept_id_std   float16 \n",
            " 18  enc_item_id_mean           float16 \n",
            " 19  enc_item_id_std            float16 \n",
            " 20  enc_item_id_state_id_mean  float16 \n",
            " 21  enc_item_id_state_id_std   float16 \n",
            " 22  enc_item_id_store_id_mean  float16 \n",
            " 23  enc_item_id_store_id_std   float16 \n",
            "dtypes: category(1), float16(22), int16(1)\n",
            "memory usage: 209.6 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qsjckIPPwk1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxUMTr9PwwB"
      },
      "source": [
        "\n",
        "Part 4 Final forecasting¶\n",
        "part 4.1 Parallelization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDlHJg12Ppmy"
      },
      "source": [
        "\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILq1vp4QAnT"
      },
      "source": [
        "#Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWTL9ftdP4wl"
      },
      "source": [
        "\n",
        "#This function load data by store ID\n",
        "def get_data_by_store(store):\n",
        "    df = pd.concat([pd.read_pickle(BASE),\n",
        "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
        "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
        "                    axis=1)\n",
        "\n",
        "    df = df[df['store_id']==store]        #only for relevant store\n",
        "\n",
        "    # Considering the memory limitations, lags and mean encoding features are \n",
        "    # Read separately, also drop the unnecessary things\n",
        "    # We can use index to keep the necessary rows\n",
        "    # Alignment is good for us as concat uses less memory than merge.\n",
        "    \n",
        "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
        "    df2 = df2[df2.index.isin(df.index)]\n",
        "    \n",
        "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
        "    df3 = df3[df3.index.isin(df.index)]\n",
        "    \n",
        "    df = pd.concat([df, df2], axis=1)\n",
        "    del df2 \n",
        "    \n",
        "    df = pd.concat([df, df3], axis=1)\n",
        "    del df3 \n",
        "\n",
        "    # Creating the list of features,\n",
        "    features = [col for col in list(df) if col not in remove_features]\n",
        "    df = df[['id','d',TARGET]+features]\n",
        "    \n",
        "    # Skipping first n rows\n",
        "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
        "    \n",
        "    return df, features\n",
        "\n",
        "#Recombine test set after the training\n",
        "def get_base_test():\n",
        "    base_test = pd.DataFrame()\n",
        "\n",
        "    for store_id in STORES_IDS:\n",
        "        temp_df = pd.read_pickle('/content/drive/My Drive/GoogleColab/AUX_MODELS/test_'+store_id+'.pkl')\n",
        "        temp_df['store_id'] = store_id\n",
        "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
        "    \n",
        "    return base_test\n",
        "\n",
        "\n",
        "#This function makes the dynamic rolling lags\n",
        "def make_lag(LAG_DAY):\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "\n",
        "def make_lag_roll(LAG_DAY):\n",
        "    shift_day = LAG_DAY[0]\n",
        "    roll_wind = LAG_DAY[1]\n",
        "    lag_df = base_test[['id','d',TARGET]]\n",
        "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
        "    return lag_df[[col_name]]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjrTtRxNQHuc"
      },
      "source": [
        "Part 4.3 Model Selection\n",
        "'boosting_type': 'gbdt' we have 'goss' option for faster training but it normally leads to underfit. Also there is good 'dart' mode but it takes forever to train and model performance depends a lot on random factor https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
        "\n",
        "'objective': 'tweedie' Tweedie Gradient Boosting for Extremely Unbalanced Zero-inflated Data https://arxiv.org/pdf/1811.10192.pdf\n",
        "\n",
        "Most of the people are using tweedie, few poisson. However we can create own loss function. https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
        "\n",
        "The'tweedie_variance_power': 1.1 (default = 1.5) if this closer to 2, shift towards a Gamma distribution and if closer to 1 shift towards a Poisson distribution.\n",
        "\n",
        "'metric': 'rmse' doesn't mean anything to us because the competition metric is different and we don't use early stoppings here. So rmse serves just for general model performance overview. https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
        "\n",
        "'subsample': 0.5 Serves to fight with overfit this will randomly select part of data without resampling Chosen by CV.\n",
        "\n",
        "'subsample_freq': 1 frequency for bagging default value - seems ok\n",
        "\n",
        "'learning_rate': 0.03 Chosen by CV Smaller - longer training but there is an option to stop in \"local minimum\" Bigger - faster training but there is a chance to not find \"global minimum\" minimum\n",
        "\n",
        "'num_leaves': 2^11-1\n",
        "\n",
        "'min_data_in_leaf': 2^12-1 Force model to use more features, we need it to reduce \"recursive\" error impact. Also it leads to overfit that's why we use small 'max_bin': 100\n",
        "\n",
        "l1, l2 regularizations l2 can work with bigger num_leaves: https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
        "\n",
        "'n_estimators': 1400 CV shows that there should be different values for each state/store. Current value was chosen for general purpose.\n",
        "\n",
        "'feature_fraction': 0.5 LightGBM will randomly select part of features on each iteration (tree). We have maaaany features and many of them are \"duplicates\" and many just \"noise\" good values here - 0.5-0.7 (by CV)\n",
        "\n",
        "'boost_from_average': False There is some \"problem\" to code boost_from_average for custom loss'True' makes training faster\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBkNwEVgP446"
      },
      "source": [
        "#Selection of model params\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',    \n",
        "                    'objective': 'tweedie',\n",
        "                    'tweedie_variance_power': 1.1,\n",
        "                    'metric': 'rmse',\n",
        "                    'subsample': 0.5,\n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.03,\n",
        "                    'num_leaves': 2**11-1,\n",
        "                    'min_data_in_leaf': 2**12-1,\n",
        "                    'feature_fraction': 0.5,\n",
        "                    'max_bin': 100,\n",
        "                    'n_estimators': 1400,\n",
        "                    'boost_from_average': False,\n",
        "                    'verbose': -1,\n",
        "                }"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfdU_Fx_QOwM"
      },
      "source": [
        "VER = 1                          # Our model version\n",
        "SEED = 42                        # We want all things\n",
        "seed_everything(SEED)            # to be as deterministic \n",
        "lgb_params['seed'] = SEED        # as possible\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "\n",
        "#LIMITS and const\n",
        "TARGET      = 'sales'            # Our target\n",
        "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
        "END_TRAIN   = 1913               # End day of our train set\n",
        "P_HORIZON   = 28                 # Prediction horizon\n",
        "USE_AUX     = True               # Use or not pretrained models\n",
        "\n",
        "\n",
        "#Features to remove: These features lead to overfit or values not present in test set\n",
        "remove_features = ['id','state_id','store_id',\n",
        "                   'date','wm_yr_wk','d',TARGET]\n",
        "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
        "                   'enc_dept_id_mean','enc_dept_id_std',\n",
        "                   'enc_item_id_mean','enc_item_id_std'] \n",
        "\n",
        "# #PATHS for Features\n",
        "# ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
        "# BASE     = '../input/m5-simple-fe/grid_part_1.pkl'\n",
        "# PRICE    = '../input/m5-simple-fe/grid_part_2.pkl'\n",
        "# CALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\n",
        "# LAGS     = '../input/m5-lags-features/lags_df_28.pkl'\n",
        "# MEAN_ENC = '../input/m5-custom-features/mean_encoding_df.pkl'\n",
        "\n",
        "#PATHS for Features\n",
        "ORIGINAL = '/content/drive/My Drive/GoogleColab/'\n",
        "BASE     = path+'grid_part_1.pkl'\n",
        "PRICE    = path+'grid_part_2.pkl'\n",
        "CALENDAR = path+'grid_part_3.pkl'\n",
        "LAGS     = path+'lags_df_28.pkl'\n",
        "MEAN_ENC = path+'mean_encoding_df.pkl'\n",
        "\n",
        "# AUX(pretrained) Models paths\n",
        "AUX_MODELS = '/content/drive/My Drive/GoogleColab/AUX_MODELS/'\n",
        "\n",
        "#STORES ids\n",
        "#STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
        "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_evaluation.csv')['store_id']\n",
        "STORES_IDS = list(STORES_IDS.unique())\n",
        "\n",
        "\n",
        "#SPLITS for lags creation\n",
        "SHIFT_DAY  = 28\n",
        "N_LAGS     = 15\n",
        "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
        "ROLS_SPLIT = []\n",
        "for i in [1,7,14]:\n",
        "    for j in [7,14,30,60]:\n",
        "        ROLS_SPLIT.append([i,j])\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB8HVEafQRtT"
      },
      "source": [
        "# The Aux Models : no need to wait for long time \n",
        "\n",
        "# if we want to use pre-trained models we can skip training\n",
        "\n",
        "if USE_AUX:\n",
        "    lgb_params['n_estimators'] = 2\n",
        "    \n",
        "# Some 'logs' that can compare\n",
        "#Train CA_1\n",
        "#[100]\tvalid_0's rmse: 2.02289\n",
        "#[200]\tvalid_0's rmse: 2.0017\n",
        "#[300]\tvalid_0's rmse: 1.99239\n",
        "#[400]\tvalid_0's rmse: 1.98471\n",
        "#[500]\tvalid_0's rmse: 1.97923\n",
        "#[600]\tvalid_0's rmse: 1.97284\n",
        "#[700]\tvalid_0's rmse: 1.96763\n",
        "#[800]\tvalid_0's rmse: 1.9624\n",
        "#[900]\tvalid_0's rmse: 1.95673\n",
        "#[1000]\tvalid_0's rmse: 1.95201\n",
        "#[1100]\tvalid_0's rmse: 1.9476\n",
        "#[1200]\tvalid_0's rmse: 1.9434\n",
        "#[1300]\tvalid_0's rmse: 1.9392\n",
        "#[1400]\tvalid_0's rmse: 1.93446"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01IZmqt7QT41"
      },
      "source": [
        "STORES_IDS=['CA_1'] # for testing purpose, only one store (CA_1) is used."
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtQjMuAYQXo0",
        "outputId": "002ad76e-93b7-4869-c34a-0df034e41777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Train models\n",
        "\n",
        "for store_id in STORES_IDS:\n",
        "    print('Train', store_id)\n",
        "    \n",
        "    # Get grid for current store\n",
        "    grid_df, features_columns = get_data_by_store(store_id)\n",
        "    \n",
        "    # Masks for \n",
        "    # Train (All data less than 1913)\n",
        "    # \"Validation\" (Last 28 days - not real validatio set)\n",
        "    # Test (All data greater than 1913 day, \n",
        "    #       with some gap for recursive features)\n",
        "    train_mask = grid_df['d']<=END_TRAIN\n",
        "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
        "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
        "    \n",
        "    # Apply masks and save lgb dataset as bin\n",
        "    # to reduce memory spikes during dtype convertations\n",
        "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
        "\n",
        "    # \"To avoid any conversions, we should always use np.float32\"\n",
        "    # or save to bin before start training\n",
        "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
        "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
        "                       label=grid_df[train_mask][TARGET])\n",
        "    train_data.save_binary('train_data.bin')\n",
        "    train_data = lgb.Dataset('train_data.bin')\n",
        "    \n",
        "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
        "                       label=grid_df[valid_mask][TARGET])\n",
        "    \n",
        "    # Saving part of the dataset for later predictions\n",
        "    # Removing features that we need to calculate recursively \n",
        "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
        "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
        "    grid_df = grid_df[keep_cols]\n",
        "    grid_df.to_pickle(path+'test_'+store_id+'.pkl')\n",
        "    del grid_df\n",
        "    \n",
        "    # Launch seeder again to make lgb training 100% deterministic\n",
        "    # with each \"code line\" np.random \"evolves\" \n",
        "    # so we need (may want) to \"reset\" it\n",
        "    seed_everything(SEED)\n",
        "    estimator = lgb.train(lgb_params,\n",
        "                          train_data,\n",
        "                          valid_sets = [valid_data],\n",
        "                          verbose_eval = 100,\n",
        "                          )\n",
        "    \n",
        "    # Save model - it's not real '.bin' but a pickle file\n",
        "    # estimator = lgb.Booster(model_file='model.txt')\n",
        "    # can only predict with the best iteration (or the saving iteration)\n",
        "    # pickle.dump gives us more flexibility\n",
        "    # like estimator.predict(TEST, num_iteration=100)\n",
        "    # num_iteration - number of iteration want to predict with, \n",
        "    # NULL or <= 0 means use best iteration\n",
        "    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
        "    pickle.dump(estimator, open(AUX_MODELS+model_name, 'wb'))\n",
        "\n",
        "    # Remove temporary files and objects \n",
        "    # to free some hdd space and ram memory\n",
        "    !rm train_data.bin\n",
        "    del train_data, valid_data, estimator\n",
        "    gc.collect()\n",
        "    \n",
        "    # \"Keep\" models features for predictions\n",
        "    MODEL_FEATURES = features_columns\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train CA_1\n",
            "[100]\tvalid_0's rmse: 1.79794\n",
            "[200]\tvalid_0's rmse: 1.82081\n",
            "[300]\tvalid_0's rmse: 1.80065\n",
            "[400]\tvalid_0's rmse: 1.78613\n",
            "[500]\tvalid_0's rmse: 1.77428\n",
            "[600]\tvalid_0's rmse: 1.7654\n",
            "[700]\tvalid_0's rmse: 1.75955\n",
            "[800]\tvalid_0's rmse: 1.75207\n",
            "[900]\tvalid_0's rmse: 1.74777\n",
            "[1000]\tvalid_0's rmse: 1.74312\n",
            "[1100]\tvalid_0's rmse: 1.73911\n",
            "[1200]\tvalid_0's rmse: 1.73365\n",
            "[1300]\tvalid_0's rmse: 1.72911\n",
            "[1400]\tvalid_0's rmse: 1.72678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sxv1BW7QcxT",
        "outputId": "3a14929b-d758-4a2b-f503-2b4ea1d5692f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "#USE_AUX=False\n",
        "#Predict Model\n",
        "\n",
        "#Create Dummy DAtaFrame to store predictions\n",
        "all_preds = pd.DataFrame()\n",
        "\n",
        "#Join back the test dataset with a small part of the training data to make recursive features\n",
        "base_test = get_base_test()\n",
        "\n",
        "#Timer to measure predictions time\n",
        "main_time = time.time()\n",
        "\n",
        "# Loop over each prediction day: as rolling lags are the most time-consuming, we\n",
        "# will calculate it for whole day,\n",
        "for PREDICT_DAY in range(1,5):    \n",
        "    print('Predict | Day:', PREDICT_DAY)\n",
        "    start_time = time.time()\n",
        "\n",
        "    #Make temporary grid to calculate rolling lags\n",
        "    grid_df = base_test.copy()\n",
        "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
        "\n",
        "    ##We need to update our target before rolling calculation, therefore, this rolling are calculated as,\n",
        "    \n",
        "# Example: If we need to predict day 1920, and we have mean rolling feature with 7 days windows\n",
        "# then, \n",
        "      # To calculate such feature we need Target for days 1913, 1914, 1915, 1916, 1917, 1918, 1919\n",
        "      # With training set in our hands we have only Target for day 1913 (Test set Target is Nan)\n",
        "      # Thats why we do recursive predictions and rolling calculations for 1914, 1915, 1916, 1917, 1918, 1919 … days.\n",
        "      # i.e Predict day 1914, then calculate rollings for 1915, and predict 1915. Again Calculate rollings for 1916, predict 1916, and so on.\n",
        "\n",
        "        \n",
        "    for store_id in STORES_IDS:\n",
        "        \n",
        "        # Read all our models and make predictions for each day/store pairs\n",
        "        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
        "        if USE_AUX:\n",
        "            model_path = AUX_MODELS + model_path\n",
        "        \n",
        "        estimator = pickle.load(open(model_path, 'rb'))\n",
        "        #estimator=pi\n",
        "        \n",
        "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
        "        store_mask = base_test['store_id']==store_id\n",
        "        \n",
        "        mask = (day_mask)&(store_mask)\n",
        "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
        "\n",
        "    #Make good column naming and dd to all_preds DataFrame\n",
        "    temp_df = base_test[day_mask][['id',TARGET]]\n",
        "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
        "    if 'id' in list(all_preds):\n",
        "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
        "    else:\n",
        "        all_preds = temp_df.copy()\n",
        "        \n",
        "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
        "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
        "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
        "    del temp_df\n",
        "    \n",
        "all_preds = all_preds.reset_index(drop=True)\n",
        "all_preds"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict | Day: 1\n",
            "##########  0.27 min round |  0.27 min total |  740.97 day sales |\n",
            "Predict | Day: 2\n",
            "##########  0.27 min round |  0.54 min total |  673.30 day sales |\n",
            "Predict | Day: 3\n",
            "##########  0.27 min round |  0.82 min total |  674.46 day sales |\n",
            "Predict | Day: 4\n",
            "##########  0.27 min round |  1.09 min total |  682.47 day sales |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.633479</td>\n",
              "      <td>0.510140</td>\n",
              "      <td>0.501746</td>\n",
              "      <td>0.495379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.633479</td>\n",
              "      <td>0.510140</td>\n",
              "      <td>0.501746</td>\n",
              "      <td>0.607546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.633479</td>\n",
              "      <td>0.510140</td>\n",
              "      <td>0.586720</td>\n",
              "      <td>0.495379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.633479</td>\n",
              "      <td>0.510140</td>\n",
              "      <td>0.586720</td>\n",
              "      <td>0.607546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
              "      <td>0.633479</td>\n",
              "      <td>0.552030</td>\n",
              "      <td>0.501746</td>\n",
              "      <td>0.495379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4715</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.931608</td>\n",
              "      <td>1.314278</td>\n",
              "      <td>1.591499</td>\n",
              "      <td>1.786904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4716</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.931608</td>\n",
              "      <td>1.534412</td>\n",
              "      <td>1.516080</td>\n",
              "      <td>1.586290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4717</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.931608</td>\n",
              "      <td>1.534412</td>\n",
              "      <td>1.516080</td>\n",
              "      <td>1.786904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4718</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.931608</td>\n",
              "      <td>1.534412</td>\n",
              "      <td>1.591499</td>\n",
              "      <td>1.586290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4719</th>\n",
              "      <td>FOODS_3_820_CA_1_validation</td>\n",
              "      <td>1.931608</td>\n",
              "      <td>1.534412</td>\n",
              "      <td>1.591499</td>\n",
              "      <td>1.786904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4720 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 id        F1        F2        F3        F4\n",
              "0     HOBBIES_1_010_CA_1_validation  0.633479  0.510140  0.501746  0.495379\n",
              "1     HOBBIES_1_010_CA_1_validation  0.633479  0.510140  0.501746  0.607546\n",
              "2     HOBBIES_1_010_CA_1_validation  0.633479  0.510140  0.586720  0.495379\n",
              "3     HOBBIES_1_010_CA_1_validation  0.633479  0.510140  0.586720  0.607546\n",
              "4     HOBBIES_1_010_CA_1_validation  0.633479  0.552030  0.501746  0.495379\n",
              "...                             ...       ...       ...       ...       ...\n",
              "4715    FOODS_3_820_CA_1_validation  1.931608  1.314278  1.591499  1.786904\n",
              "4716    FOODS_3_820_CA_1_validation  1.931608  1.534412  1.516080  1.586290\n",
              "4717    FOODS_3_820_CA_1_validation  1.931608  1.534412  1.516080  1.786904\n",
              "4718    FOODS_3_820_CA_1_validation  1.931608  1.534412  1.591499  1.586290\n",
              "4719    FOODS_3_820_CA_1_validation  1.931608  1.534412  1.591499  1.786904\n",
              "\n",
              "[4720 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbDbEXo_YYZH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}